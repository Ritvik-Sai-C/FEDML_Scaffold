{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "xdzjRF_p7rhl"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "# python libraties\n",
    "import os, cv2,itertools\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from glob import glob\n",
    "from PIL import Image\n",
    "import pickle\n",
    "\n",
    "# pytorch libraries\n",
    "import torch\n",
    "from torch import optim,nn\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader,Dataset\n",
    "from torchvision import models,transforms\n",
    "\n",
    "# sklearn libraries\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# to make the results are reproducible\n",
    "np.random.seed(10)\n",
    "torch.manual_seed(10)\n",
    "torch.cuda.manual_seed(10)\n",
    "\n",
    "print(os.listdir(\"/u/student/2020/cs20btech11046/resnet/input\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EazUlvA2TXHF",
    "outputId": "1cf7f76a-a995-4454-c834-1fa4b23a71b6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# set random seeds (this is used to ensure that you get the same randomness everytime no matter how many times you run the code)\n",
    "np.random.seed(10)\n",
    "torch.manual_seed(10)\n",
    "torch.cuda.manual_seed(10)\n",
    "\n",
    "# set device (if gpu/cuda is available perform the neural network operations using that else use a cpu)\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(\"| using device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "VESLoY55Tag-"
   },
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "bsz = 10  #batch sizebb\n",
    "no_clients = 10 #no.of clients\n",
    "epsilon = 1e-10 #used in scaffold_experiment function (not sure what formula is used)\n",
    "lr = 0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '/u/student/2020/cs20btech11046/resnet/input/skin-cancer-mnist-ham10000'\n",
    "all_image_path = glob(os.path.join(data_dir, '*', '*.jpg'))\n",
    "imageid_path_dict = {os.path.splitext(os.path.basename(x))[0]: x for x in all_image_path}\n",
    "lesion_type_dict = {\n",
    "    'nv': 'Melanocytic nevi',\n",
    "    'mel': 'dermatofibroma',\n",
    "    'bkl': 'Benign keratosis-like lesions ',\n",
    "    'bcc': 'Basal cell carcinoma',\n",
    "    'akiec': 'Actinic keratoses',\n",
    "    'vasc': 'Vascular lesions',\n",
    "    'df': 'Dermatofibroma'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_img_mean_std(image_paths):\n",
    "    \"\"\"\n",
    "        computing the mean and std of three channel on the whole dataset,\n",
    "        first we should normalize the image from 0-255 to 0-1\n",
    "    \"\"\"\n",
    "\n",
    "    img_h, img_w = 224, 224\n",
    "    imgs = []\n",
    "    means, stdevs = [], []\n",
    "\n",
    "    for i in tqdm(range(len(image_paths))):\n",
    "        img = cv2.imread(image_paths[i])\n",
    "        img = cv2.resize(img, (img_h, img_w))\n",
    "        imgs.append(img)\n",
    "\n",
    "    imgs = np.stack(imgs, axis=3)\n",
    "    print(imgs.shape)\n",
    "\n",
    "    imgs = imgs.astype(np.float32) / 255.\n",
    "\n",
    "    for i in range(3):\n",
    "        pixels = imgs[:, :, i, :].ravel()  # resize to one row\n",
    "        means.append(np.mean(pixels))\n",
    "        stdevs.append(np.std(pixels))\n",
    "\n",
    "    means.reverse()  # BGR --> RGB\n",
    "    stdevs.reverse()\n",
    "\n",
    "    print(\"normMean = {}\".format(means))\n",
    "    print(\"normStd = {}\".format(stdevs))\n",
    "    return means,stdevs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████| 20030/20030 [03:01<00:00, 110.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(224, 224, 3, 20030)\n",
      "normMean = [0.7630444, 0.5456509, 0.57003975]\n",
      "normStd = [0.14092743, 0.15261324, 0.16997053]\n"
     ]
    }
   ],
   "source": [
    "norm_mean,norm_std = compute_img_mean_std(all_image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "A-BmfL-mhirk"
   },
   "outputs": [],
   "source": [
    "#a class NonIIDMyDataset is created to access and transform data\n",
    "class NonIIDMyDataset(Dataset):\n",
    "    #the __init__ function in Python is like the C++ constructor in an object-oriented approach\n",
    "    def __init__(self, data, targets, transform=None):\n",
    "        self.data = data  #data is X\n",
    "        self.targets = torch.LongTensor(targets)  #tragets are y. Convert y to a tensor to be able to used torch function on them\n",
    "        self.transform = transform  #this is the transformation to be applied on to X. By default the value is None.\n",
    "                                    #In the 2nd cell below you can see the exact transform used in the code\n",
    "\n",
    "    #this function is used to apply a transformation (if any) to X and return the pair (X, y) based on the index passed\n",
    "    def __getitem__(self, index):\n",
    "        x = self.data[index]\n",
    "        y = self.targets[index]\n",
    "\n",
    "        if self.transform:\n",
    "            # x = Image.fromarray(self.data[index].astype(np.uint8).transpose(1,2,0))\n",
    "            x = self.transform(x)\n",
    "\n",
    "        return x, y\n",
    "\n",
    "    #this function is used to get the length/no.of features of X\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "#The functions with __ at the front and back of every function in Python are called Magic Methods/Dunder Methods.\n",
    "#Magic methods are not meant to be invoked directly by you, but the invocation happens internally from the class on a certain action.\n",
    "#Not sure of the meaning but may be this concept is understood better if you find where these methods are used int he code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################################\n",
    "from torch.optim import Optimizer\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "class AlgoOptimizer(Optimizer):\n",
    "    def __init__(self, params, lr, weight_decay):\n",
    "        defaults = dict(lr=lr, weight_decay=weight_decay)\n",
    "        super(AlgoOptimizer, self).__init__(params, defaults)\n",
    "\n",
    "    def step(self, main_controls, client_controls, closure=None):\n",
    "\n",
    "        loss = None\n",
    "        if closure is not None:\n",
    "            loss = closure\n",
    "\n",
    "        for group in self.param_groups:\n",
    "            for p, c, ci in zip(group['params'], main_controls.values(), client_controls.values()):\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "                dp = p.grad.data + c.data - ci.data\n",
    "                p.data = p.data - dp.data * 0.01\n",
    "\n",
    "        return loss\n",
    "######################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################################\n",
    "def save_object(obj, filename):\n",
    "    with open(filename, \"wb\") as fp:\n",
    "        pickle.dump(obj, fp, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "def read_object(filename):\n",
    "    with open(filename, \"rb\") as fp:\n",
    "        obj = pickle.load(fp)\n",
    "\n",
    "    return obj\n",
    "######################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "WRYOEMIuNpBe"
   },
   "outputs": [],
   "source": [
    "#these are the locations of train and test data for 20 clients used in the code\n",
    "#there are many other folders as well in this dataset folder. May be different ones are used for different cases\n",
    "train_location = '/u/student/2020/cs20btech11046/resnet/old/dataset/practical/10/train/'\n",
    "test_location = '/u/student/2020/cs20btech11046/resnet/old/dataset/practical/10/test/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "stJUm9zgevMl"
   },
   "outputs": [],
   "source": [
    "#the transforms library imported above, is used to create a transformation of the data(X)\n",
    "#transforms.Compose - to put more than one sequantial transforms into one\n",
    "#transforms.ToTensor - to convert a list/np array to a tensor\n",
    "#transform.Normalize - transforms.Normalize(mean, std, inplace=False) to normalize a tensor with give mean and std\n",
    "#to normalize a data means changinf x to (x-mean)/std\n",
    "#here mean is 0.137 and std is 0.3081. May be these values are obtained by calculating mean and std of X separately or they are random. Not sure\n",
    "\n",
    "#how did these value we got\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "])\n",
    "\n",
    "#this function converts y of data to a tensor using __init__, applies the above transformation to x using the __getitenm__ function in the NonIIDMyDataset\n",
    "#and loads the data in batches (in order to train it with a neural network later) and stores the loaded data into client_loader list created above\n",
    "def noniid_train_loader(bsz=10):\n",
    "    client_loader_train, client_loader_val = [], []  #this list is used to store the train data loaded using 'DataLoader' module from torch in batches\n",
    "\n",
    "    #for all the no_clients clients\n",
    "    for i in range(no_clients):\n",
    "        #go to the folder /content/drive/MyDrive/dataset/practical/<no_clients>/train/, read the file from client_num.npz (liek 1.npz, 2.npz ... 20.npz) and store the X and y values\n",
    "        file_path = str(i)+'.npz'\n",
    "        loc = train_location + file_path\n",
    "        data = np.load(loc)\n",
    "        X = list(data['features'])\n",
    "        Y = list(data['labels'])\n",
    "\n",
    "        #create an object called dataset which is an instance of the class NonIIDMyDataset\n",
    "        dataset = NonIIDMyDataset(X, Y, transform=transform)\n",
    "\n",
    "        dataset_train, dataset_val = torch.utils.data.random_split(dataset, [0.8, 0.2])\n",
    "        #in batches of 10, load the whole dataset and store it in client_load\n",
    "        client_load_train = torch.utils.data.DataLoader(dataset_train, batch_size=bsz, shuffle=True)\n",
    "        client_load_val = torch.utils.data.DataLoader(dataset_val, batch_size=bsz, shuffle=True)\n",
    "\n",
    "        #append every client's dataload into client_loader list\n",
    "        client_loader_train.append(client_load_train)\n",
    "        client_loader_val.append(client_load_val)\n",
    "\n",
    "    print(client_loader_train, client_loader_val)  #you can see <no_clients> objects of torch dataloaders\n",
    "    return client_loader_train, client_loader_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zehfiWjBfCeA",
    "outputId": "d7fc28de-a0ba-4828-9231-3a7fde909831",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<torch.utils.data.dataloader.DataLoader object at 0x7fa04f246310>, <torch.utils.data.dataloader.DataLoader object at 0x7fa04ef4a290>, <torch.utils.data.dataloader.DataLoader object at 0x7fa04ea5f850>, <torch.utils.data.dataloader.DataLoader object at 0x7fa04ea8af50>, <torch.utils.data.dataloader.DataLoader object at 0x7fa04ea8b250>, <torch.utils.data.dataloader.DataLoader object at 0x7fa04ea8b6d0>, <torch.utils.data.dataloader.DataLoader object at 0x7fa04ea8ba90>, <torch.utils.data.dataloader.DataLoader object at 0x7fa04ea8bf90>, <torch.utils.data.dataloader.DataLoader object at 0x7fa04ea5fc10>, <torch.utils.data.dataloader.DataLoader object at 0x7fa02ef4c690>] [<torch.utils.data.dataloader.DataLoader object at 0x7fa04ea7b410>, <torch.utils.data.dataloader.DataLoader object at 0x7fa04ee78850>, <torch.utils.data.dataloader.DataLoader object at 0x7fa04ea8ac50>, <torch.utils.data.dataloader.DataLoader object at 0x7fa04ea8b050>, <torch.utils.data.dataloader.DataLoader object at 0x7fa04ea8b3d0>, <torch.utils.data.dataloader.DataLoader object at 0x7fa04ea8b7d0>, <torch.utils.data.dataloader.DataLoader object at 0x7fa04ea8bb90>, <torch.utils.data.dataloader.DataLoader object at 0x7fa04ea8bd90>, <torch.utils.data.dataloader.DataLoader object at 0x7fa02ef4c2d0>, <torch.utils.data.dataloader.DataLoader object at 0x7fa02ef4c490>]\n"
     ]
    }
   ],
   "source": [
    "[noniid_client_train_loader, noniid_client_val_loader] = noniid_train_loader(bsz = bsz) #call the above funtion to perform all the actions explained inside the func, noniid_train_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "N628K2e5M8HK"
   },
   "outputs": [],
   "source": [
    "#the exact same thing as in the func noniid_train_loader is done here. Expect that the data is extracted now read from the loacation /content/drive/MyDrive/dataset/practical/<no_clients>/test\n",
    "test_loader = []\n",
    "def noniid_test_loader(batch_size,shuffle):\n",
    "    for i in range(no_clients):\n",
    "        file_path = str(i)+'.npz'\n",
    "        loc = test_location + file_path\n",
    "        data = np.load(loc)\n",
    "        X = list(data['features'])\n",
    "        Y = list(data['labels'])\n",
    "\n",
    "        dataset = NonIIDMyDataset(X, Y, transform=transform)\n",
    "        client_load = torch.utils.data.DataLoader(dataset, batch_size=bsz, shuffle=True)\n",
    "\n",
    "        test_loader.append(client_load)\n",
    "\n",
    "    print(test_loader)\n",
    "    return test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uij69lidL_1b",
    "outputId": "8c86f2ac-b7d3-4b56-e468-9bcbe74e1d75"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<torch.utils.data.dataloader.DataLoader object at 0x7fa04ea78190>, <torch.utils.data.dataloader.DataLoader object at 0x7fa02ef4dad0>, <torch.utils.data.dataloader.DataLoader object at 0x7fa02ef4e410>, <torch.utils.data.dataloader.DataLoader object at 0x7fa02ef4e810>, <torch.utils.data.dataloader.DataLoader object at 0x7fa02ef4cbd0>, <torch.utils.data.dataloader.DataLoader object at 0x7fa04ef4bb10>, <torch.utils.data.dataloader.DataLoader object at 0x7fa02ef4ef10>, <torch.utils.data.dataloader.DataLoader object at 0x7fa02ef4f090>, <torch.utils.data.dataloader.DataLoader object at 0x7fa02ef4f250>, <torch.utils.data.dataloader.DataLoader object at 0x7fa02ef4f410>]\n"
     ]
    }
   ],
   "source": [
    "test_loader = noniid_test_loader(batch_size = 1000, shuffle=False)  #test data is tranformed loaded in batches of 1000 and stored in test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uvdAGMQpfKj1",
    "outputId": "5b513ea5-1c45-412a-d011-db15a59f71d4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.utils.data.dataloader.DataLoader'>\n",
      "<torch.utils.data.dataloader.DataLoader object at 0x7fa04f246310>\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'F' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m#using a for-loop, count the no.of rows is dataset which has 10 classes respectively for client 1\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m (x,y) \u001b[38;5;129;01min\u001b[39;00m noniid_client_train_loader[\u001b[38;5;241m0\u001b[39m]:\n\u001b[0;32m----> 9\u001b[0m     label_dist\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msum(F\u001b[38;5;241m.\u001b[39mone_hot(y,num_classes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)  \u001b[38;5;66;03m#one-hot encoding is explained int he next cell\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-iid: \u001b[39m\u001b[38;5;124m\"\u001b[39m, label_dist)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m#I suppose there should be a line like label_dist = torch.zeros(10) here as well\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'F' is not defined"
     ]
    }
   ],
   "source": [
    "# non-iid\n",
    "#this cell is totally just for observation\n",
    "label_dist = torch.zeros(10)  #since we have 10 classes, create a torch array with 10 zeros\n",
    "print(type(noniid_client_train_loader[0]))\n",
    "print(noniid_client_train_loader[0])\n",
    "\n",
    "#using a for-loop, count the no.of rows is dataset which has 10 classes respectively for client 1\n",
    "for (x,y) in noniid_client_train_loader[0]:\n",
    "    label_dist+= torch.sum(F.one_hot(y,num_classes=10), dim=0)  #one-hot encoding is explained int he next cell\n",
    "\n",
    "print(\"non-iid: \", label_dist)\n",
    "#I suppose there should be a line like label_dist = torch.zeros(10) here as well\n",
    "for (x,y) in test_loader[0]:\n",
    "\n",
    "    label_dist+= torch.sum(F.one_hot(y,num_classes=10), dim=0)\n",
    "\n",
    "print(\"non-iid: \", label_dist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IlmXWOTb0c7u"
   },
   "source": [
    "'''\n",
    "one hot encoding is a concept where we assign 1 for the class of that row and 0 for the rest\n",
    "example say we have 5 classes in the dataset.\n",
    "The classes of say 10 rows of data are 1 3 2 4 1 5 3 2 1 4. (i.e., 1st row of data belongs to class 1 ...)\n",
    "After applying one hot encoding the classes of these 10 rows will be represented as\n",
    "1th row : 1 0 0 0 0\n",
    "2th row : 0 0 1 0 0\n",
    "3th row : 0 1 0 0 0\n",
    "4th row : 0 0 0 1 0\n",
    "5th row : 1 0 0 0 0\n",
    "6th row : 0 0 0 0 1\n",
    "7th row : 0 0 1 0 0\n",
    "8th row : 0 1 0 0 0\n",
    "9th row : 1 0 0 0 0\n",
    "10th row: 0 0 0 1 0\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LPO0winNG0Vn"
   },
   "outputs": [],
   "source": [
    "#this function is only used to observe how many parameters are used in the neural network we create. It is only for observation. Not to effect the running of any code\n",
    "#parameters in neural networks are like no.of weights or bias params included to the network. Check it out on the internet\n",
    "def num_params(model):\n",
    "    \"\"\" \"\"\"\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "q6THHwG8k8tQ",
    "outputId": "24fc07ed-44f7-4c66-f666-11dcf2833918"
   },
   "outputs": [],
   "source": [
    "# define cnn\n",
    "#A CNN (Convolutional Neural Network) is another kind of NN.\n",
    "#In MLPs, there are layers like linear layers where a linear operation like y = w.T*x+b is applied (a linear operation) followed by activation\n",
    "#Similarly, in CNN, as the name suggests, convolution is done on x (input) to get y (output) on some layers. Here kernels are used.\n",
    "#I suggest you to look through some blogs and understand practically and mathematically\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, 5)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 5)\n",
    "        self.fc = nn.Linear(1024, 512)\n",
    "        self.out = nn.Linear(512, 10)\n",
    "        self.loss = 0\n",
    "        self.len = 0\n",
    "        self.control = {}\n",
    "        self.delta_control = {}\n",
    "        self.delta_y = {}\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.max_pool2d(self.conv1(x), 2, 2) # [B x 32 x 12 x 12]\n",
    "        x = F.max_pool2d(self.conv2(x), 2, 2) # [B x 64 x 4 x 4]\n",
    "        x = x.flatten(1) # [B x 1024]\n",
    "        x = F.relu(self.fc(x)) # [B x 512]\n",
    "        x = self.out(x) # [B x 10]\n",
    "        return x\n",
    "\n",
    "print(CNN())\n",
    "print(num_params(CNN()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5bMA-Huh4RGO"
   },
   "source": [
    "'''\n",
    "The whole idea of neural network and data revolves around the below steps:\n",
    "1. Create a basic neural network be it MLP, CNN, RNN\n",
    "2. Transform & Normalize data to be able to train and validate data using the network\n",
    "3. Change the weights etc., parameters of the neural network through back propogation or any other method\n",
    "4. For the same choose a loss function and an optimizer.\n",
    "5. Repeat until you reach some fixed no.of iterations or desired result\n",
    "\n",
    "So basically train your network with initial weights and the data and predict ŷ.\n",
    "Calculate loss/error using the loss func you choose. An example is (y-ŷ).\n",
    "If the error is more, re-train the network with new weights. This is done through back propgation which is automatically done most of the times.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pkcLDjnalBgd"
   },
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss() #the loss function we chose is cross entropy. The mathematical formula is available on the internet\n",
    "\n",
    "#the below function is used to validate (find the percentage of correctly predicted output)\n",
    "def validate(model, client_loader):\n",
    "    #change the model/network to evaluation mode and for the given client, predict ŷ = model(x). If ŷ=y, add 1 to correct\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for (t, (x,y)) in enumerate(client_loader):\n",
    "            x = x.to(device)\n",
    "            x = x.permute(0, 2, 3, 1)\n",
    "            #print(\"x\",x.shape)\n",
    "            y = y.to(device)\n",
    "            out = model(x)\n",
    "            _, predicted = torch.max(out, 1)\n",
    "            total += y.size(0)\n",
    "            correct += (predicted == y).sum().item()\n",
    "            #correct += torch.sum(torch.argmax(out, dim=1) == y).item()\n",
    "            #total += x.shape[0]\n",
    "    return correct/total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_fv2VaSF9X5Q"
   },
   "source": [
    "'''\n",
    "Basically what is happening above is that, create a state_dict called aggregated_state_dict and intitialize it with our present client's data\n",
    "Now run a for loop through all the other client's model and add their params (state_dict) to the aggregated_state_dict\n",
    "Then to normalize it, divide all the params of aggreated_state_dict by no.of clients (20 here)\n",
    "Then create a model structure for aggregated_model and load all these params into this.\n",
    "\n",
    "But ig we can just add the params of all clients directly instead of that if it!=client_no statement. Cz at the end I feel we are just adding the params of all the local models.\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "######################################################################\n",
    "def test(cnn):\n",
    "        cnn.eval()\n",
    "        _, _, Dte = nn_seq_wind(cnn.name, cnn.B)\n",
    "        pred = []\n",
    "        y = []\n",
    "        for (seq, target) in tqdm(Dte):\n",
    "            with torch.no_grad():\n",
    "                seq = seq.to(device)\n",
    "                y_pred = cnn(seq)\n",
    "                pred.extend(list(chain.from_iterable(y_pred.data.tolist())))\n",
    "                y.extend(list(chain.from_iterable(target.data.tolist())))\n",
    "\n",
    "        pred = np.array(pred)\n",
    "        y = np.array(y)\n",
    "        print(\"mae: \", mean_absolute_error(y, pred), \"rmse: \", np.sqrt(mean_squared_error(y, pred)))\n",
    "    \n",
    "    print(\"\\n\\n-------------------Testing the final model on all the clients-------------------\\n\\n\")\n",
    "    \n",
    "    model = read_object(\"./main.pkl\")\n",
    "    model.eval()\n",
    "    \n",
    "    c = clients\n",
    "    for client in c:\n",
    "        model.name = client\n",
    "        test(model)\n",
    "######################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "######################################################################\n",
    "def train(cnn, main, client_loader_train, client_loader_val):\n",
    "    cnn.train()\n",
    "    cnn.len = len(client_loader_train)\n",
    "    \n",
    "    print(\"-------------------------------Training the Data-------------------------------\")\n",
    "   \n",
    "    cnn_copy = copy.deepcopy(cnn)\n",
    "    optimizer = AlgoOptimizer(cnn.parameters(), lr=lr, weight_decay=1e-5)\n",
    "    lr_step = StepLR(optimizer, step_size=20, gamma=0.0001)\n",
    "    # training\n",
    "    min_epochs = 10\n",
    "    best_model = None\n",
    "    min_val_loss = 5\n",
    "    \n",
    "    for epoch in tqdm(range(K)):\n",
    "        train_loss = []\n",
    "        for (i, (x,y)) in enumerate(client_loader_train):\n",
    "            x = x.to(device)\n",
    "            x = x.permute(0, 2, 3, 1)\n",
    "            \n",
    "            y = y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            out = cnn(x)\n",
    "            loss = criterion(out, y)\n",
    "            train_loss.append(loss.item())\n",
    "            loss.backward()\n",
    "            optimizer.step(main.control, cnn.control)\n",
    "            \n",
    "        # validation\n",
    "#         cnn.eval()\n",
    "#         val_loss = 0.0\n",
    "#         with torch.no_grad():\n",
    "#             for (i, (x,y)) in enumerate(client_loader_val):\n",
    "#                 x = x.to(device)\n",
    "#                 x = x.permute(0, 2, 3, 1)\n",
    "\n",
    "#                 y = y.to(device)\n",
    "#                 out = cnn(x)\n",
    "#                 loss = criterion(out, y)\n",
    "\n",
    "#                 val_loss += loss.item()\n",
    "\n",
    "#         val_loss /= len(client_loader_val)\n",
    "        \n",
    "#         if epoch + 1 >= min_epochs and val_loss < min_val_loss:\n",
    "#             min_val_loss = val_loss\n",
    "#             best_model = copy.deepcopy(cnn)\n",
    "\n",
    "        print('epoch {:01d} train_loss {:.8f} '.format(epoch, np.mean(train_loss)))\n",
    "        #cnn.train()\n",
    "\n",
    "    temp = {}\n",
    "    for k, v in cnn.named_parameters():\n",
    "        temp[k] = v.data.clone()\n",
    "\n",
    "    for k, v in cnn_copy.named_parameters():\n",
    "        local_steps = K * len(client_loader_train)\n",
    "        cnn.control[k] = cnn.control[k] - main.control[k] + (v.data - temp[k]) / (local_steps * 0.01)\n",
    "        cnn.delta_y[k] = temp[k] - v.data\n",
    "        cnn.delta_control[k] = cnn.control[k] - cnn_copy.control[k]\n",
    "\n",
    "    return cnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WBF3a_JRlKur"
   },
   "outputs": [],
   "source": [
    "def aggregation(N, main):\n",
    "    delta_x = {}\n",
    "    delta_c = {}\n",
    "    \n",
    "    for k, v in main.named_parameters():\n",
    "        delta_x[k] = torch.zeros_like(v.data)\n",
    "        delta_c[k] = torch.zeros_like(v.data)\n",
    "\n",
    "    for i in range(N):\n",
    "        client = read_object(\"./clients/client\"+str(i)+\".pkl\")\n",
    "        for k, v in client.named_parameters():\n",
    "            delta_x[k] += client.delta_y[k] / N  # averaging\n",
    "            delta_c[k] += client.delta_control[k] / N  # averaging\n",
    "\n",
    "    Ng = 1\n",
    "    for k, v in main.named_parameters():\n",
    "        v.data += (Ng)*delta_x[k].data\n",
    "        main.control[k].data += delta_c[k].data * (N / N)\n",
    "\n",
    "    return main\n",
    "\n",
    "def caller(client_no, round_no):\n",
    "    cnn = CNN().to(device)\n",
    "\n",
    "    for k, v in cnn.named_parameters():\n",
    "        cnn.control[k] = torch.zeros_like(v.data)\n",
    "        cnn.delta_control[k] = torch.zeros_like(v.data)\n",
    "        cnn.delta_y[k] = torch.zeros_like(v.data)\n",
    "\n",
    "    if round_no == 0:\n",
    "        main = read_object(\"./main.pkl\")\n",
    "    else:\n",
    "        main = read_object(\"./clients/client\"+str(client_no)+\"_main.pkl\")\n",
    "        main = aggregation(N, main)\n",
    "\n",
    "    save_object(main, \"./clients/client\"+str(client_no)+\"_main.pkl\")\n",
    "\n",
    "    cnn = train(cnn, main, noniid_client_train_loader[client_no], noniid_client_val_loader[client_no])\n",
    "\n",
    "    save_object(cnn, \"./clients/client\"+str(client_no)+\".pkl\")\n",
    "        \n",
    "    if client_no == N-1 and round_no == R-1:\n",
    "        main = read_object(\"./main.pkl\")\n",
    "        main = aggregation(N, main)\n",
    "        save_object(main, \"./main.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "######################################################################\n",
    "# N: No.of clients\n",
    "# Cper: Percentage of clients to be chosen for every communication round\n",
    "# K: No.of update steps in the clients\n",
    "# B: Batch size\n",
    "# R: No.of communication rounds\n",
    "# input_dim: Dimension of the input\n",
    "# lr: learning rate\n",
    "\n",
    "N, Cper, K, B, R = 10, 0.5, 10, 50, 10\n",
    "\n",
    "clients = []\n",
    "for task in range(1, 2):\n",
    "    for zone in range(1, 11):\n",
    "        clients.append(\"Task\" + str(task) + \"_W_Zone\" + str(zone))\n",
    "\n",
    "cnn = CNN().to(device)\n",
    "\n",
    "for k, v in cnn.named_parameters():\n",
    "    cnn.control[k] = torch.zeros_like(v.data)\n",
    "    cnn.delta_control[k] = torch.zeros_like(v.data)\n",
    "    cnn.delta_y[k] = torch.zeros_like(v.data)\n",
    "\n",
    "save_object(cnn, \"./main.pkl\")\n",
    "\n",
    "round_acc = []\n",
    "\n",
    "for r in range(R):\n",
    "    print(\"-----------------------------------Round \" + str(r+1) + \"-----------------------------------\")\n",
    "\n",
    "    for i in range(N):\n",
    "        print(\"-----------------------------------Client \" + str(i+1) + \"-----------------------------------\")\n",
    "        caller(i, r)\n",
    "        \n",
    "    acc_val = 0\n",
    "    for client_no in range(N):\n",
    "        model = read_object(\"./clients/client\"+str(client_no)+\"_main.pkl\")\n",
    "        val_acc = validate(model, noniid_client_val_loader[client_no])\n",
    "        acc_val = acc_val + val_acc\n",
    "        print(\"client {}, validation acc: {}\".format(client_no+1, val_acc))\n",
    "        round_acc.append(val_acc)\n",
    "        \n",
    "    acc_val = acc_val/no_clients\n",
    "    \n",
    "    save_object(round_acc, \"./round_\"+str(r+1)+\"_acc.pkl\")\n",
    "    print(\"client accuracies after round \", r, \" \", round_acc)\n",
    "    print('round_acc {} '.format(acc_val))\n",
    "    \n",
    "    round_acc = []\n",
    "            \n",
    "    \n",
    "######################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F8EdqdNVI5sV"
   },
   "source": [
    "x = np.arange(0,15)\n",
    "plt.figure(figsize=(8,6))\n",
    "\n",
    "plt.title(\"Scaffold test accuracy after $t$ rounds on non-iid MNIST\")\n",
    "\n",
    "plt.xlabel(\"Communication rounds $t$\")\n",
    "plt.ylabel(\"Test accuracy\")\n",
    "plt.axis([0, 15, 0.3, 1])\n",
    "\n",
    "\n",
    "\n",
    "plt.axhline(y=0.7, color='r', linestyle='dashed')\n",
    "plt.axhline(y=0.9, color='b', linestyle='dashed')\n",
    "\n",
    "plt.plot(x, acc_cnn_noniid_r10_ep10, label='2NN, $m=10$, $E=1$')\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
