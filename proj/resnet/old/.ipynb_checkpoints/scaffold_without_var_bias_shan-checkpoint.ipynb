{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "xdzjRF_p7rhl"
   },
   "outputs": [],
   "source": [
    "#import necessary libraries for the code\n",
    "import numpy as np  #used mainly to save and load data from .npz files (our data in the drive is also stored in this format)\n",
    "import matplotlib.pyplot as plt #used for plotting or data visualization. In this code the part is all commented (the last 3-4 cells)\n",
    "import copy #used to make deepcopy of a neural network (meaning of deepcopy is explained in the cell below)\n",
    "import IPython  #IPython is an interactive command-line terminal for Python (but it is not used in any of the running code)\n",
    "from PIL import Image #used to perform tasks with images (but it is not used in running code)\n",
    "import pickle\n",
    "\n",
    "#every library below is used to tp create, train and evaluate the neural networks we create for our data\n",
    "#the use of each library is explained when it is used in the code\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "from torch.optim.lr_scheduler import StepLR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EazUlvA2TXHF",
    "outputId": "1cf7f76a-a995-4454-c834-1fa4b23a71b6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# set random seeds (this is used to ensure that you get the same randomness everytime no matter how many times you run the code)\n",
    "np.random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "\n",
    "# set device (if gpu/cuda is available perform the neural network operations using that else use a cpu)\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(\"| using device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "VESLoY55Tag-"
   },
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "bsz = 10  #batch size\n",
    "no_clients = 3 #no.of clients\n",
    "epsilon = 1e-10 #used in scaffold_experiment function (not sure what formula is used)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "A-BmfL-mhirk"
   },
   "outputs": [],
   "source": [
    "#a class NonIIDMyDataset is created to access and transform data\n",
    "class NonIIDMyDataset(Dataset):\n",
    "    #the __init__ function in Python is like the C++ constructor in an object-oriented approach\n",
    "    def __init__(self, data, targets, transform=None):\n",
    "        self.data = data  #data is X\n",
    "        self.targets = torch.LongTensor(targets)  #tragets are y. Convert y to a tensor to be able to used torch function on them\n",
    "        self.transform = transform  #this is the transformation to be applied on to X. By default the value is None.\n",
    "                                    #In the 2nd cell below you can see the exact transform used in the code\n",
    "\n",
    "    #this function is used to apply a transformation (if any) to X and return the pair (X, y) based on the index passed\n",
    "    def __getitem__(self, index):\n",
    "        x = self.data[index]\n",
    "        y = self.targets[index]\n",
    "\n",
    "        if self.transform:\n",
    "            # x = Image.fromarray(self.data[index].astype(np.uint8).transpose(1,2,0))\n",
    "            x = self.transform(x)\n",
    "\n",
    "        return x, y\n",
    "\n",
    "    #this function is used to get the length/no.of features of X\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "#The functions with __ at the front and back of every function in Python are called Magic Methods/Dunder Methods.\n",
    "#Magic methods are not meant to be invoked directly by you, but the invocation happens internally from the class on a certain action.\n",
    "#Not sure of the meaning but may be this concept is understood better if you find where these methods are used int he code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################################\n",
    "from torch.optim import Optimizer\n",
    "\n",
    "class AlgoOptimizer(Optimizer):\n",
    "    def __init__(self, params, lr, weight_decay):\n",
    "        defaults = dict(lr=lr, weight_decay=weight_decay)\n",
    "        super(AlgoOptimizer, self).__init__(params, defaults)\n",
    "\n",
    "    def step(self, main_controls, client_controls, closure=None):\n",
    "\n",
    "        loss = None\n",
    "        if closure is not None:\n",
    "            loss = closure\n",
    "\n",
    "        for group in self.param_groups:\n",
    "            for p, c, ci in zip(group['params'], main_controls.values(), client_controls.values()):\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "                dp = p.grad.data + c.data - ci.data\n",
    "                p.data = p.data - dp.data * group[\"lr\"]\n",
    "\n",
    "        return loss\n",
    "######################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################################\n",
    "import pandas as pd\n",
    "import torch\n",
    "import pickle\n",
    "\n",
    "sys.path.append('../')\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def save_object(obj, filename):\n",
    "    with open(filename, \"wb\") as fp:\n",
    "        pickle.dump(obj, fp, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "def read_object(filename):\n",
    "    with open(filename, \"rb\") as fp:\n",
    "        obj = pickle.load(fp)\n",
    "\n",
    "    return obj\n",
    "######################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "WRYOEMIuNpBe"
   },
   "outputs": [],
   "source": [
    "#these are the locations of train and test data for 20 clients used in the code\n",
    "#there are many other folders as well in this dataset folder. May be different ones are used for different cases\n",
    "train_location = '/u/student/2020/cs20btech11046/resnet/old/dataset/practical/20/train/'\n",
    "test_location = '/u/student/2020/cs20btech11046/resnet/old/dataset/practical/20/test/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "stJUm9zgevMl"
   },
   "outputs": [],
   "source": [
    "#the transforms library imported above, is used to create a transformation of the data(X)\n",
    "#transforms.Compose - to put more than one sequantial transforms into one\n",
    "#transforms.ToTensor - to convert a list/np array to a tensor\n",
    "#transform.Normalize - transforms.Normalize(mean, std, inplace=False) to normalize a tensor with give mean and std\n",
    "#to normalize a data means changinf x to (x-mean)/std\n",
    "#here mean is 0.137 and std is 0.3081. May be these values are obtained by calculating mean and std of X separately or they are random. Not sure\n",
    "\n",
    "#how did these value we got\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "])\n",
    "\n",
    "client_loader = []  #this list is used to store the train data loaded using 'DataLoader' module from torch in batches\n",
    "\n",
    "#this function converts y of data to a tensor using __init__, applies the above transformation to x using the __getitenm__ function in the NonIIDMyDataset\n",
    "#and loads the data in batches (in order to train it with a neural network later) and stores the loaded data into client_loader list created above\n",
    "def noniid_train_loader(bsz=10):\n",
    "  #for all the 20 clients\n",
    "  for i in range(no_clients):\n",
    "    #go to the folder /content/drive/MyDrive/dataset/practical/20/train/, read the file from client_num.npz (liek 1.npz, 2.npz ... 20.npz) and store the X and y values\n",
    "    file_path = str(i)+'.npz'\n",
    "    loc = train_location + file_path\n",
    "    data = np.load(loc)\n",
    "    X = list(data['features'])\n",
    "    Y = list(data['labels'])\n",
    "\n",
    "    #create an object called dataset which is an instance of the class NonIIDMyDataset\n",
    "    dataset = NonIIDMyDataset(X, Y, transform=transform)\n",
    "    #in batches of 10, load the whole dataset and store it in client_load\n",
    "    client_load = torch.utils.data.DataLoader(dataset, batch_size=bsz, shuffle=True)\n",
    "\n",
    "    #append every client's dataload into client_loader list\n",
    "    client_loader.append(client_load)\n",
    "\n",
    "  print(client_loader)  #you can see 20 objects of torch dataloaders\n",
    "  return client_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zehfiWjBfCeA",
    "outputId": "d7fc28de-a0ba-4828-9231-3a7fde909831"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<torch.utils.data.dataloader.DataLoader object at 0x7fa2f1892bd0>, <torch.utils.data.dataloader.DataLoader object at 0x7fa2f1892610>, <torch.utils.data.dataloader.DataLoader object at 0x7fa2f18a5790>, <torch.utils.data.dataloader.DataLoader object at 0x7fa2f18b7c50>, <torch.utils.data.dataloader.DataLoader object at 0x7fa2f18b7e50>, <torch.utils.data.dataloader.DataLoader object at 0x7fa2f18b6490>, <torch.utils.data.dataloader.DataLoader object at 0x7fa2ea56c3d0>, <torch.utils.data.dataloader.DataLoader object at 0x7fa2ea56c590>, <torch.utils.data.dataloader.DataLoader object at 0x7fa2f1892e50>, <torch.utils.data.dataloader.DataLoader object at 0x7fa2ea56ca90>]\n"
     ]
    }
   ],
   "source": [
    "noniid_client_train_loader = noniid_train_loader(bsz = bsz) #call the above funtion to perform all the actions explained inside the func, noniid_train_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "N628K2e5M8HK"
   },
   "outputs": [],
   "source": [
    "#the exact same thing as in the func noniid_train_loader is done here. Expect that the data is extracted now read from the loacation /content/drive/MyDrive/dataset/practical/20/test\n",
    "test_loader = []\n",
    "def noniid_test_loader(batch_size,shuffle):\n",
    "  for i in range(no_clients):\n",
    "    file_path = str(i)+'.npz'\n",
    "    loc = test_location + file_path\n",
    "    data = np.load(loc)\n",
    "    X = list(data['features'])\n",
    "    Y = list(data['labels'])\n",
    "\n",
    "    dataset = NonIIDMyDataset(X, Y, transform=transform)\n",
    "    client_load = torch.utils.data.DataLoader(dataset, batch_size=bsz, shuffle=True)\n",
    "\n",
    "    test_loader.append(client_load)\n",
    "\n",
    "  print(test_loader)\n",
    "  return test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uij69lidL_1b",
    "outputId": "8c86f2ac-b7d3-4b56-e468-9bcbe74e1d75"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<torch.utils.data.dataloader.DataLoader object at 0x7fa2ea56e090>, <torch.utils.data.dataloader.DataLoader object at 0x7fa2ea56df50>, <torch.utils.data.dataloader.DataLoader object at 0x7fa2f18b4dd0>, <torch.utils.data.dataloader.DataLoader object at 0x7fa2ea56dfd0>, <torch.utils.data.dataloader.DataLoader object at 0x7fa2ea56e990>, <torch.utils.data.dataloader.DataLoader object at 0x7fa2ea56eb50>, <torch.utils.data.dataloader.DataLoader object at 0x7fa2f18b54d0>, <torch.utils.data.dataloader.DataLoader object at 0x7fa2ea56ef10>, <torch.utils.data.dataloader.DataLoader object at 0x7fa2ea56f090>, <torch.utils.data.dataloader.DataLoader object at 0x7fa2ea56f250>]\n"
     ]
    }
   ],
   "source": [
    "test_loader = noniid_test_loader(batch_size = 1000, shuffle=False)  #test data is tranformed loaded in batches of 1000 and stored in test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uvdAGMQpfKj1",
    "outputId": "5b513ea5-1c45-412a-d011-db15a59f71d4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.utils.data.dataloader.DataLoader'>\n",
      "<torch.utils.data.dataloader.DataLoader object at 0x7fa2f1892bd0>\n",
      "non-iid:  tensor([101., 662.,   0.,   0.,   1., 245.,   0.,  23., 796., 144.])\n",
      "non-iid:  tensor([ 333.,  663.,    0.,    0., 4344.,  245.,    0.,   23.,  796.,  145.])\n"
     ]
    }
   ],
   "source": [
    "# non-iid\n",
    "#this cell is totally just for observation\n",
    "label_dist = torch.zeros(10)  #since we have 10 classes, create a torch array with 10 zeros\n",
    "print(type(noniid_client_train_loader[0]))\n",
    "print(noniid_client_train_loader[0])\n",
    "\n",
    "#using a for-loop, count the no.of rows is dataset which has 10 classes respectively for client 1\n",
    "for (x,y) in noniid_client_train_loader[0]:\n",
    "    label_dist+= torch.sum(F.one_hot(y,num_classes=10), dim=0)  #one-hot encoding is explained int he next cell\n",
    "\n",
    "print(\"non-iid: \", label_dist)\n",
    "#I suppose there should be a line like label_dist = torch.zeros(10) here as well\n",
    "for (x,y) in test_loader[0]:\n",
    "\n",
    "    label_dist+= torch.sum(F.one_hot(y,num_classes=10), dim=0)\n",
    "\n",
    "print(\"non-iid: \", label_dist)\n",
    "# fig, axes = plt.subplots(1, 2, figsize=(10, 4))\n",
    "# for i, ax in enumerate(axes.flat):\n",
    "#     ax.axis(\"off\")\n",
    "#     ax.set_title(label[i].cpu().numpy())\n",
    "#     ax.imshow(img[i][0], cmap=\"gray\")\n",
    "# IPython.display.display(fig)\n",
    "# plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "IlmXWOTb0c7u"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\none hot encoding is a concept where we assign 1 for the class of that row and 0 for the rest\\nexample say we have 5 classes in the dataset.\\nThe classes of say 10 rows of data are 1 3 2 4 1 5 3 2 1 4. (i.e., 1st row of data belongs to class 1 ...)\\nAfter applying one hot encoding the classes of these 10 rows will be represented as\\n1th row : 1 0 0 0 0\\n2th row : 0 0 1 0 0\\n3th row : 0 1 0 0 0\\n4th row : 0 0 0 1 0\\n5th row : 1 0 0 0 0\\n6th row : 0 0 0 0 1\\n7th row : 0 0 1 0 0\\n8th row : 0 1 0 0 0\\n9th row : 1 0 0 0 0\\n10th row: 0 0 0 1 0\\n'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "one hot encoding is a concept where we assign 1 for the class of that row and 0 for the rest\n",
    "example say we have 5 classes in the dataset.\n",
    "The classes of say 10 rows of data are 1 3 2 4 1 5 3 2 1 4. (i.e., 1st row of data belongs to class 1 ...)\n",
    "After applying one hot encoding the classes of these 10 rows will be represented as\n",
    "1th row : 1 0 0 0 0\n",
    "2th row : 0 0 1 0 0\n",
    "3th row : 0 1 0 0 0\n",
    "4th row : 0 0 0 1 0\n",
    "5th row : 1 0 0 0 0\n",
    "6th row : 0 0 0 0 1\n",
    "7th row : 0 0 1 0 0\n",
    "8th row : 0 1 0 0 0\n",
    "9th row : 1 0 0 0 0\n",
    "10th row: 0 0 0 1 0\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "LPO0winNG0Vn"
   },
   "outputs": [],
   "source": [
    "#this function is only used to observe how many parameters are used in the neural network we create. It is only for observation. Not to effect the running of any code\n",
    "#parameters in neural networks are like no.of weights or bias params included to the network. Check it out on the internet\n",
    "def num_params(model):\n",
    "    \"\"\" \"\"\"\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "q6THHwG8k8tQ",
    "outputId": "24fc07ed-44f7-4c66-f666-11dcf2833918"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN(\n",
      "  (conv1): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (fc): Linear(in_features=1024, out_features=512, bias=True)\n",
      "  (out): Linear(in_features=512, out_features=10, bias=True)\n",
      ")\n",
      "582026\n"
     ]
    }
   ],
   "source": [
    "# define cnn\n",
    "#A CNN (Convolutional Neural Network) is another kind of NN.\n",
    "#In MLPs, there are layers like linear layers where a linear operation like y = w.T*x+b is applied (a linear operation) followed by activation\n",
    "#Similarly, in CNN, as the name suggests, convolution is done on x (input) to get y (output) on some layers. Here kernels are used.\n",
    "#I suggest you to look through some blogs and understand practically and mathematically\n",
    "\n",
    "#the network below is: input -> convolution 2D layer -> max pool activation -> conv 2d -> max pool -> linear -> relu -> linear -> output\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, name, B, K):\n",
    "        super(ANN, self).__init__()\n",
    "        self.name = name\n",
    "        self.B = B\n",
    "        self.K = K\n",
    "        self.len = 0\n",
    "        self.lr = 0.01\n",
    "        self.loss = 0\n",
    "        self.control = {}\n",
    "        self.delta_control = {}\n",
    "        self.delta_y = {}\n",
    "        self.conv1 = nn.Conv2d(1, 32, 5)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 5)\n",
    "        self.fc = nn.Linear(1024, 512)\n",
    "        self.out = nn.Linear(512, 10)\n",
    "        \n",
    "    def forward(self, data):\n",
    "        x = F.max_pool2d(self.conv1(data), 2, 2) # [B x 32 x 12 x 12]\n",
    "        x = F.max_pool2d(self.conv2(x), 2, 2) # [B x 64 x 4 x 4]\n",
    "        x = x.flatten(1) # [B x 1024]\n",
    "        x = F.relu(self.fc(x)) # [B x 512]\n",
    "        x = self.out(x) # [B x 10]\n",
    "        \n",
    "        return x\n",
    "\n",
    "#     def __init__(self):\n",
    "#         super(CNN, self).__init__()\n",
    "#         self.conv1 = nn.Conv2d(1, 32, 5)\n",
    "#         self.conv2 = nn.Conv2d(32, 64, 5)\n",
    "#         self.fc = nn.Linear(1024, 512)\n",
    "#         self.out = nn.Linear(512, 10)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = F.max_pool2d(self.conv1(x), 2, 2) # [B x 32 x 12 x 12]\n",
    "#         x = F.max_pool2d(self.conv2(x), 2, 2) # [B x 64 x 4 x 4]\n",
    "#         x = x.flatten(1) # [B x 1024]\n",
    "#         x = F.relu(self.fc(x)) # [B x 512]\n",
    "#         x = self.out(x) # [B x 10]\n",
    "#         return x\n",
    "\n",
    "print(CNN())\n",
    "print(num_params(CNN()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "5bMA-Huh4RGO"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nThe whole idea of neural network and data revolves around the below steps:\\n1. Create a basic neural network be it MLP, CNN, RNN\\n2. Transform & Normalize data to be able to train and validate data using the network\\n3. Change the weights etc., parameters of the neural network through back propogation or any other method\\n4. For the same choose a loss function and an optimizer.\\n5. Repeat until you reach some fixed no.of iterations or desired result\\n\\nSo basically train your network with initial weights and the data and predict ŷ.\\nCalculate loss/error using the loss func you choose. An example is (y-ŷ).\\nIf the error is more, re-train the network with new weights. This is done through back propgation which is automatically done most of the times.\\n'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "The whole idea of neural network and data revolves around the below steps:\n",
    "1. Create a basic neural network be it MLP, CNN, RNN\n",
    "2. Transform & Normalize data to be able to train and validate data using the network\n",
    "3. Change the weights etc., parameters of the neural network through back propogation or any other method\n",
    "4. For the same choose a loss function and an optimizer.\n",
    "5. Repeat until you reach some fixed no.of iterations or desired result\n",
    "\n",
    "So basically train your network with initial weights and the data and predict ŷ.\n",
    "Calculate loss/error using the loss func you choose. An example is (y-ŷ).\n",
    "If the error is more, re-train the network with new weights. This is done through back propgation which is automatically done most of the times.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################################\n",
    "def get_val_loss(model, Val):\n",
    "    model.eval()\n",
    "    loss_function = torch.nn.MSELoss().to(device)\n",
    "    val_loss = []\n",
    "    for (seq, label) in Val:\n",
    "        with torch.no_grad():\n",
    "            seq = seq.to(device)\n",
    "            label = label.to(device)\n",
    "            y_pred = model(seq)\n",
    "            loss = loss_function(y_pred, label)\n",
    "            val_loss.append(loss.item())\n",
    "\n",
    "    return np.mean(val_loss)\n",
    "######################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "pkcLDjnalBgd"
   },
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss() #the loss function we chose is cross entropy. The mathematical formula is available on the internet\n",
    "\n",
    "#the below function is used to validate (find the percentage of correctly predicted output)\n",
    "def validate(model,client):\n",
    "    #change the model/network to evaluation mode and for the given client, predict ŷ = model(x). If ŷ=y, add 1 to correct\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for (t, (x,y)) in enumerate(test_loader[client]):\n",
    "            x = x.to(device)\n",
    "            x = x.permute(0, 2, 3, 1)\n",
    "            #print(\"x\",x.shape)\n",
    "            y = y.to(device)\n",
    "            out = model(x)\n",
    "            correct += torch.sum(torch.argmax(out, dim=1) == y).item()\n",
    "            total += x.shape[0]\n",
    "    return correct/total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################################\n",
    "def train(ann, main):\n",
    "    ann.train()\n",
    "    Dtr, Val, Dte = nn_seq_wind(ann.name, ann.B)\n",
    "    ann.len = len(Dtr)\n",
    "    \n",
    "    print(\"-------------------------------Training the Data-------------------------------\")\n",
    "       \n",
    "    loss_function = torch.nn.MSELoss().to(device)\n",
    "    x = copy.deepcopy(ann)\n",
    "    optimizer = AlgoOptimizer(ann.parameters(), lr=ann.lr, weight_decay=1e-5)\n",
    "    lr_step = StepLR(optimizer, step_size=20, gamma=0.0001)\n",
    "    # training\n",
    "    min_epochs = 10\n",
    "    best_model = None\n",
    "    min_val_loss = 5\n",
    "    for epoch in tqdm(range(ann.K)):\n",
    "        train_loss = []\n",
    "        for (seq, label) in Dtr:\n",
    "            seq = seq.to(device)\n",
    "            label = label.to(device)\n",
    "            y_pred = ann.forward(seq)\n",
    "            loss = loss_function(y_pred, label)\n",
    "            train_loss.append(loss.item())\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step(main.control, ann.control)\n",
    "        lr_step.step()\n",
    "        # validation\n",
    "        val_loss = get_val_loss(ann, Val)\n",
    "        if epoch + 1 >= min_epochs and val_loss < min_val_loss:\n",
    "            min_val_loss = val_loss\n",
    "            best_model = copy.deepcopy(ann)\n",
    "\n",
    "        print('epoch {:03d} train_loss {:.8f} val_loss {:.8f}'.format(epoch, np.mean(train_loss), val_loss))\n",
    "        ann.train()\n",
    "\n",
    "    ann = copy.deepcopy(best_model)\n",
    "    temp = {}\n",
    "    for k, v in ann.named_parameters():\n",
    "        temp[k] = v.data.clone()\n",
    "\n",
    "    for k, v in x.named_parameters():\n",
    "        local_steps = ann.K * len(Dtr)\n",
    "        ann.control[k] = ann.control[k] - main.control[k] + (v.data - temp[k]) / (local_steps * ann.lr)\n",
    "        ann.delta_y[k] = temp[k] - v.data\n",
    "        ann.delta_control[k] = ann.control[k] - x.control[k]\n",
    "\n",
    "    return ann\n",
    "\n",
    "def aggregation(N, main):\n",
    "    delta_x = {}\n",
    "    delta_c = {}\n",
    "    \n",
    "    for k, v in main.named_parameters():\n",
    "        delta_x[k] = torch.zeros_like(v.data)\n",
    "        delta_c[k] = torch.zeros_like(v.data)\n",
    "\n",
    "    for i in range(N):\n",
    "        client = read_object(\"clients/client\"+str(i)+\".pkl\")\n",
    "        for k, v in client.named_parameters():\n",
    "            delta_x[k] += client.delta_y[k] / N  # averaging\n",
    "            delta_c[k] += client.delta_control[k] / N  # averaging\n",
    "\n",
    "    Ng = 1\n",
    "    for k, v in main.named_parameters():\n",
    "        v.data += (Ng)*delta_x[k].data\n",
    "        main.control[k].data += delta_c[k].data * (N / N)\n",
    "\n",
    "    return main\n",
    "\n",
    "def caller(client_no, round_no):\n",
    "    params = read_object(\"defaults.pkl\")\n",
    "    \n",
    "    nn = ANN(input_dim = params[\"input_dim\"], name = params[\"clients\"][client_no], B = params[\"B\"], K = params[\"K\"], lr = params[\"lr\"]).to(device)\n",
    "\n",
    "    for k, v in nn.named_parameters():\n",
    "        nn.control[k] = torch.zeros_like(v.data)\n",
    "        nn.delta_control[k] = torch.zeros_like(v.data)\n",
    "        nn.delta_y[k] = torch.zeros_like(v.data)\n",
    "\n",
    "    if round_no == 0:\n",
    "        main = read_object(\"main.pkl\")\n",
    "    else:\n",
    "        main = read_object(\"clients/client\"+str(client_no)+\"_main.pkl\")\n",
    "        main = aggregation(params[\"N\"], main)\n",
    "\n",
    "    save_object(main, \"clients/client\"+str(client_no)+\"_main.pkl\")\n",
    "\n",
    "    nn = train(nn, main)\n",
    "\n",
    "    save_object(nn, \"clients/client\"+str(client_no)+\".pkl\")\n",
    "\n",
    "    if client_no == 9 and round_no == 9:\n",
    "        main = read_object(\"main.pkl\")\n",
    "        main = aggregation(params[\"N\"], main)\n",
    "        save_object(main, \"main.pkl\")\n",
    "######################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "WBF3a_JRlKur"
   },
   "outputs": [],
   "source": [
    "def train_client(id, client_loader, global_model, num_local_epochs, lr):\n",
    "    #create a deepcopy of the global model and change the network to train mode\n",
    "    local_model = copy.deepcopy(global_model)\n",
    "    local_model = local_model.to(device)\n",
    "    local_model.train()\n",
    "    #optimizer = torch.optim.SGD(local_model.parameters(), lr=lr)\n",
    "    optimizer = AlgoOptimizer(local_model.parameters(), lr=lr, weight_decay=1e-5)\n",
    "    \n",
    "    #for given no.of epochs (iterations) run the for-loop on the client\n",
    "    for epoch in range(num_local_epochs):\n",
    "        #for every pair of (X, y), predict ŷ for X using the local model, find the loss (using cross entropy loss here) which is l\n",
    "        #predict ŷ for X using the global model, find the loss with this ŷ and y which is dl\n",
    "        #I guess this is some new formula in this version. find the total loss for present (X, y) using the formula\n",
    "        #(1-p)*loss + p*drift_loss where p = l/(l+dl+epsilon)\n",
    "        #with this loss, as said in the 2nd cell above, we perform back propogation and optimize.\n",
    "        #I am exactly not sure of the math or steps which happen in back prop and optimization\n",
    "        #You can check it online. You may get an idea\n",
    "        for (i, (x,y)) in enumerate(client_loader):\n",
    "            x = x.to(device)\n",
    "\n",
    "            #x.reshape(10, 1, 28, 28)\n",
    "            x = x.permute(0, 2, 3, 1)\n",
    "            #print(\"x\",x.shape)\n",
    "            y = y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            out = local_model(x)\n",
    "            loss = criterion(out, y)\n",
    "            global_output_batch = global_model(x)\n",
    "            drift_loss = criterion(global_output_batch,y)\n",
    "            l = loss.item()#print('loss',loss.item())\n",
    "            dl = drift_loss.item()\n",
    "            #print('drift loss',drift_loss.item())\n",
    "            p = l/(l+dl+ epsilon)\n",
    "            #print(p)\n",
    "            total_loss = (1-p)* loss+ p*drift_loss\n",
    "            total_loss.backward()\n",
    "            #optimizer.step()\n",
    "            optimizer.step(global_model.control, local_model.control)\n",
    "            #optimizer.step(main.control, ann.control)\n",
    "        #lr_step.step()\n",
    "\n",
    "    #after training the local model with the client's data for given no.of epochs, we return the local model\n",
    "    #So here, the overall model (like the server model) as effects the weights/params of local model\n",
    "    #cx every client has their own global model and local model like in our prev sem code\n",
    "    #remember we used to approach to remove the usage of server completely?\n",
    "    return local_model\n",
    "\n",
    "#this func is same as aggregation func in the prev sems code. models argument is an array of local models of all the clients\n",
    "def running_model_avg(models,client_no):\n",
    "    # Initialize the aggregated model's state dictionary\n",
    "    aggregated_state_dict = models[client_no].state_dict()\n",
    "    it = 0\n",
    "    # Iterate over the models of the clients\n",
    "    for client_model in models:\n",
    "        #if the clinet_model is not the one passed as an argument\n",
    "        if it!=client_no:\n",
    "          # Get the state dictionary of the current client model\n",
    "          client_state_dict = client_model.state_dict()\n",
    "\n",
    "          # Iterate over the parameters in the state dictionary\n",
    "          for param_name, param in client_state_dict.items():\n",
    "              # Perform aggregation for each parameter\n",
    "              if param_name.endswith(\".weight\") or param_name.endswith(\".bias\"):\n",
    "                  # Update the aggregated parameter by averaging\n",
    "                  aggregated_state_dict[param_name] += param\n",
    "        it = it + 1\n",
    "\n",
    "    # Compute the average by dividing by the number of client models\n",
    "    num_client_models = len(models)\n",
    "    for param_name in aggregated_state_dict:\n",
    "        aggregated_state_dict[param_name] /= num_client_models\n",
    "\n",
    "    # Create a new model instance for the aggregated model\n",
    "    aggregated_model = type(models[0])()\n",
    "    aggregated_model.load_state_dict(aggregated_state_dict)\n",
    "\n",
    "    return aggregated_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "_fv2VaSF9X5Q"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nBasically what is happening above is that, create a state_dict called aggregated_state_dict and intitialize it with our present client's data\\nNow run a for loop through all the other client's model and add their params (state_dict) to the aggregated_state_dict\\nThen to normalize it, divide all the params of aggreated_state_dict by no.of clients (20 here)\\nThen create a model structure for aggregated_model and load all these params into this.\\n\\nBut ig we can just add the params of all clients directly instead of that if it!=client_no statement. Cz at the end I feel we are just adding the params of all the local models.\\n\""
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Basically what is happening above is that, create a state_dict called aggregated_state_dict and intitialize it with our present client's data\n",
    "Now run a for loop through all the other client's model and add their params (state_dict) to the aggregated_state_dict\n",
    "Then to normalize it, divide all the params of aggreated_state_dict by no.of clients (20 here)\n",
    "Then create a model structure for aggregated_model and load all these params into this.\n",
    "\n",
    "But ig we can just add the params of all clients directly instead of that if it!=client_no statement. Cz at the end I feel we are just adding the params of all the local models.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "Cw0lZVFflOSL"
   },
   "outputs": [],
   "source": [
    "#modified scaffold each client having their own global model global_model[0] for client[0]\n",
    "\n",
    "def scaffold_experiment(global_model, num_clients, num_local_epochs, lr, client_train_loader, max_rounds, filename):   ##num_client\n",
    "    round_accuracy = []\n",
    "    #for all the rounds\n",
    "    for t in range(max_rounds):\n",
    "        print(\"starting round {}\".format(t))\n",
    "\n",
    "        # choose clients\n",
    "        #clients = np.random.choice(np.arange(100), num_clients, replace = False)  ###remove this\n",
    "        clients=num_clients\n",
    "        print(\"clients: \", clients)   ##\n",
    "        #create 2 list to store global (aggregated) and local models of each client\n",
    "        running_avg = [None for _ in range(clients)]\n",
    "        local_models = [None for _ in range(clients)]\n",
    "\n",
    "        for i in range(clients):\n",
    "          global_model[i].eval()\n",
    "          global_model[i] = global_model[i].to(device)\n",
    "          #running_avg = np.empty(clients, dtype='collections.OrderedDict') #None\n",
    "\n",
    "        #for all the clients, train their local models with their dataset\n",
    "        for i in range(clients):\n",
    "            # train local client\n",
    "            print(\"round {}, starting client {}/{}, id: {}\".format(t, i+1,num_clients, i+1))\n",
    "            local_models[i] = train_client(i, client_train_loader[i], global_model[i], num_local_epochs, lr)\n",
    "\n",
    "        # add local model parameters to running average of each client\n",
    "        for j in range(clients):\n",
    "          #print(type(running_avg[j]), '   and ',type(local_model.state_dict()))\n",
    "          global_model[j] = running_model_avg(local_models,j)\n",
    "\n",
    "\n",
    "       # validate\n",
    "        acc_val = 0\n",
    "        for client in range(no_clients):\n",
    "          val_acc = validate(global_model[client],client)\n",
    "          #print('each client',val_acc)\n",
    "          acc_val = acc_val + val_acc\n",
    "        acc_val = acc_val/no_clients\n",
    "        print(\"round {}, validation acc: {}\".format(t, val_acc))\n",
    "        round_accuracy.append(acc_val)\n",
    "\n",
    "        if (t % 10 == 0):\n",
    "          np.save(filename+'_{}'.format(t)+'.npy', np.array(round_accuracy))\n",
    "\n",
    "    return np.array(round_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################################\n",
    "def test(ann):\n",
    "        ann.eval()\n",
    "        _, _, Dte = nn_seq_wind(ann.name, ann.B)\n",
    "        pred = []\n",
    "        y = []\n",
    "        for (seq, target) in tqdm(Dte):\n",
    "            with torch.no_grad():\n",
    "                seq = seq.to(device)\n",
    "                y_pred = ann(seq)\n",
    "                pred.extend(list(chain.from_iterable(y_pred.data.tolist())))\n",
    "                y.extend(list(chain.from_iterable(target.data.tolist())))\n",
    "\n",
    "        pred = np.array(pred)\n",
    "        y = np.array(y)\n",
    "        print(\"mae: \", mean_absolute_error(y, pred), \"rmse: \", np.sqrt(mean_squared_error(y, pred)))\n",
    "    \n",
    "    print(\"\\n\\n-------------------Testing the final model on all the clients-------------------\\n\\n\")\n",
    "    \n",
    "    model = read_object(\"main.pkl\")\n",
    "    model.eval()\n",
    "    \n",
    "    c = clients\n",
    "    for client in c:\n",
    "        model.name = client\n",
    "        test(model)\n",
    "######################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################################\n",
    "\n",
    "def main():\n",
    "    # N: No.of clients\n",
    "    # Cper: Percentage of clients to be chosen for every communication round\n",
    "    # K: No.of update steps in the clients\n",
    "    # B: Batch size\n",
    "    # R: No.of communication rounds\n",
    "    # input_dim: Dimension of the input\n",
    "    # lr: learning rate\n",
    "\n",
    "    N, Cper, K, B, R = 10, 0.5, 10, 50, 10\n",
    "    input_dim = 28\n",
    "    lr = 0.08\n",
    "\n",
    "    clients = []\n",
    "    for task in range(1, 2):\n",
    "        for zone in range(1, 11):\n",
    "            clients.append(\"Task\" + str(task) + \"_W_Zone\" + str(zone))\n",
    "\n",
    "    params = {\"N\": N, \"Cper\": Cper, \"K\": K, \"B\": B, \"R\": R, \"clients\": clients, \"input_dim\": input_dim, \"lr\": lr}\n",
    "\n",
    "    # save dictionary of default parameters into a .pkl file\n",
    "    save_object(params, \"defaults.pkl\")\n",
    "        \n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    nn = CNN(name = \"main\", B = B, K = K).to(device)\n",
    "\n",
    "    for k, v in nn.named_parameters():\n",
    "        nn.control[k] = torch.zeros_like(v.data)\n",
    "        nn.delta_control[k] = torch.zeros_like(v.data)\n",
    "        nn.delta_y[k] = torch.zeros_like(v.data)\n",
    "\n",
    "    save_object(nn, \"main.pkl\")\n",
    "\n",
    "    for r in range(R):\n",
    "        print(\"-----------------------------------Round \" + str(r+1) + \"-----------------------------------\")\n",
    "    \n",
    "        for i in range(N):\n",
    "            print(\"-----------------------------------Client \" + str(i) + \"-----------------------------------\")\n",
    "            caller(i, r)\n",
    "            \n",
    "######################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Kj2J-t3vlUPX",
    "outputId": "8b932176-23a2-4ba7-a360-5378f82ece90"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN(\n",
      "  (conv1): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (fc): Linear(in_features=1024, out_features=512, bias=True)\n",
      "  (out): Linear(in_features=512, out_features=10, bias=True)\n",
      ")\n",
      "total params:  582026\n",
      "starting round 0\n",
      "clients:  10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/u/student/2020/cs20btech11046/anaconda3/lib/python3.11/site-packages/torch/cuda/__init__.py:611: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "round 0, starting client 1/10, id: 1\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'CNN' object has no attribute 'control'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 17\u001b[0m\n\u001b[1;32m     11\u001b[0m   cnn_noniid_r10_ep10[i] \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mdeepcopy(cnn)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m#cnn_iid_m10 = copy.deepcopy(cnn)\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m#calculate the accuracies obtained by the final aggregated global model for all the client's datasets\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m#we are passing, global models of all clients, no.of clients, no.of epochs, learning rate for optimizer,\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m#the train data of all clients loaded (at the beginning of this notebook), no.of rounds to run the exp, filepath to store the results\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m acc_cnn_noniid_r10_ep10 \u001b[38;5;241m=\u001b[39m scaffold_experiment(cnn_noniid_r10_ep10, num_clients\u001b[38;5;241m=\u001b[39mno_clients,\n\u001b[1;32m     18\u001b[0m                                  num_local_epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m,\n\u001b[1;32m     19\u001b[0m                                  lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.01\u001b[39m,\n\u001b[1;32m     20\u001b[0m                                  client_train_loader \u001b[38;5;241m=\u001b[39m noniid_client_train_loader,\n\u001b[1;32m     21\u001b[0m                                  max_rounds\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m,  \u001b[38;5;66;03m## for testing keep it 1\u001b[39;00m\n\u001b[1;32m     22\u001b[0m                                  filename\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/u/student/2020/cs20btech11046/resnet/old/dataset/results/acc_cnn_noniid_clt20_e20_biasvar\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28mprint\u001b[39m(acc_cnn_noniid_r10_ep10)\n\u001b[1;32m     24\u001b[0m np\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/u/student/2020/cs20btech11046/resnet/old/dataset/results/acc_cnn_noniid_clt20_ep20_biasvar.npy\u001b[39m\u001b[38;5;124m'\u001b[39m, acc_cnn_noniid_r10_ep10)\n",
      "Cell \u001b[0;32mIn[22], line 26\u001b[0m, in \u001b[0;36mscaffold_experiment\u001b[0;34m(global_model, num_clients, num_local_epochs, lr, client_train_loader, max_rounds, filename)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(clients):\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;66;03m# train local client\u001b[39;00m\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mround \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, starting client \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, id: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(t, i\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m,num_clients, i\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m---> 26\u001b[0m     local_models[i] \u001b[38;5;241m=\u001b[39m train_client(i, client_train_loader[i], global_model[i], num_local_epochs, lr)\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# add local model parameters to running average of each client\u001b[39;00m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(clients):\n\u001b[1;32m     30\u001b[0m   \u001b[38;5;66;03m#print(type(running_avg[j]), '   and ',type(local_model.state_dict()))\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[20], line 38\u001b[0m, in \u001b[0;36mtrain_client\u001b[0;34m(id, client_loader, global_model, num_local_epochs, lr)\u001b[0m\n\u001b[1;32m     36\u001b[0m         total_loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     37\u001b[0m         \u001b[38;5;66;03m#optimizer.step()\u001b[39;00m\n\u001b[0;32m---> 38\u001b[0m         optimizer\u001b[38;5;241m.\u001b[39mstep(global_model\u001b[38;5;241m.\u001b[39mcontrol, local_model\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[1;32m     39\u001b[0m         \u001b[38;5;66;03m#optimizer.step(main.control, ann.control)\u001b[39;00m\n\u001b[1;32m     40\u001b[0m     \u001b[38;5;66;03m#lr_step.step()\u001b[39;00m\n\u001b[1;32m     41\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;66;03m#cx every client has their own global model and local model like in our prev sem code\u001b[39;00m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;66;03m#remember we used to approach to remove the usage of server completely?\u001b[39;00m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m local_model\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1695\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1693\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[1;32m   1694\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[0;32m-> 1695\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'CNN' object has no attribute 'control'"
     ]
    }
   ],
   "source": [
    "#cnn modified cnn_iid_m10 for each client seperate\n",
    "cnn = CNN() #create a CNN object\n",
    "print(cnn)\n",
    "print(\"total params: \", num_params(cnn))\n",
    "# CNN - iid - m=10 experiment\n",
    "cnn_noniid_r10_ep10 = np.empty(no_clients, dtype=CNN) #create a CNN with uninitialized and random params (weights and bias) with size 20\n",
    "\n",
    "#for all the 20 clients, make a deepcopy of cnn created in the first line of this cell (initializing the empty array created above)\n",
    "#these will be the global models for all the 20 clients\n",
    "for i in range(no_clients):\n",
    "  cnn_noniid_r10_ep10[i] = copy.deepcopy(cnn)\n",
    "\n",
    "#cnn_iid_m10 = copy.deepcopy(cnn)\n",
    "#calculate the accuracies obtained by the final aggregated global model for all the client's datasets\n",
    "#we are passing, global models of all clients, no.of clients, no.of epochs, learning rate for optimizer,\n",
    "#the train data of all clients loaded (at the beginning of this notebook), no.of rounds to run the exp, filepath to store the results\n",
    "acc_cnn_noniid_r10_ep10 = scaffold_experiment(cnn_noniid_r10_ep10, num_clients=no_clients,\n",
    "                                 num_local_epochs=5,\n",
    "                                 lr=0.01,\n",
    "                                 client_train_loader = noniid_client_train_loader,\n",
    "                                 max_rounds=10,  ## for testing keep it 1\n",
    "                                 filename='/u/student/2020/cs20btech11046/resnet/old/dataset/results/acc_cnn_noniid_clt20_e20_biasvar')\n",
    "print(acc_cnn_noniid_r10_ep10)\n",
    "np.save('/u/student/2020/cs20btech11046/resnet/old/dataset/results/acc_cnn_noniid_clt20_ep20_biasvar.npy', acc_cnn_noniid_r10_ep10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gaAEiZsdTHwD"
   },
   "outputs": [],
   "source": [
    "#this function is not use anywhere in the code\n",
    "def view_10(img, label):\n",
    "    \"\"\" view 10 labelled examples from tensor\"\"\"\n",
    "    fig, axes = plt.subplots(2, 5, figsize=(10, 4))\n",
    "    for i, ax in enumerate(axes.flat):\n",
    "        ax.axis(\"off\")\n",
    "        ax.set_title(label[i].cpu().numpy())\n",
    "        ax.imshow(img[i][0], cmap=\"gray\")\n",
    "    IPython.display.display(fig)\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "smplc98wIWKv"
   },
   "outputs": [],
   "source": [
    " acc_cnn_noniid_r10_ep10 = np.load('/u/student/2020/cs20btech11046/resnet/old/dataset/results/acc_cnn_noniid_r10_ep10_bias9.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F8EdqdNVI5sV"
   },
   "outputs": [],
   "source": [
    "x = np.arange(0,15)\n",
    "plt.figure(figsize=(8,6))\n",
    "\n",
    "plt.title(\"Scaffold test accuracy after $t$ rounds on non-iid MNIST\")\n",
    "\n",
    "plt.xlabel(\"Communication rounds $t$\")\n",
    "plt.ylabel(\"Test accuracy\")\n",
    "plt.axis([0, 15, 0.3, 1])\n",
    "\n",
    "\n",
    "\n",
    "plt.axhline(y=0.7, color='r', linestyle='dashed')\n",
    "plt.axhline(y=0.9, color='b', linestyle='dashed')\n",
    "\n",
    "plt.plot(x, acc_cnn_noniid_r10_ep10, label='2NN, $m=10$, $E=1$')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eOSMoo4W6Y2c"
   },
   "outputs": [],
   "source": [
    "acc_cnn_noniid_r10_ep10_b = np.load('/u/student/2020/cs20btech11046/resnet/old/dataset/results/acc_cnn_noniid_r10_ep10_bias8.npy')\n",
    "acc_cnn_noniid_r10_ep10_wb = np.load('/u/student/2020/cs20btech11046/resnet/old/dataset/results/acc_cnn_noniid_r10_ep10_nobias.npy')\n",
    "acc_cnn_noniid_r10_ep10_b9 = np.load('/u/student/2020/cs20btech11046/resnet/old/dataset/results/acc_cnn_noniid_r10_ep10_biasvar.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6BEq4HJb6egL"
   },
   "outputs": [],
   "source": [
    "x = np.arange(0,15)\n",
    "plt.figure(figsize=(8,6))\n",
    "\n",
    "plt.title(\"Scaffold test accuracy after $t$ rounds on non-iid MNIST\")\n",
    "\n",
    "plt.xlabel(\"Communication rounds $t$\")\n",
    "plt.ylabel(\"Test accuracy\")\n",
    "plt.axis([0, 15, 0.3, 1])\n",
    "\n",
    "\n",
    "\n",
    "plt.axhline(y=0.5, color='r', linestyle='dashed')\n",
    "plt.axhline(y=0.9, color='b', linestyle='dashed')\n",
    "\n",
    "graph1, =plt.plot(x, acc_cnn_noniid_r10_ep10_b, label='b8')\n",
    "graph2, =plt.plot(x, acc_cnn_noniid_r10_ep10_wb, label='wb')\n",
    "graph3, =plt.plot(x, acc_cnn_noniid_r10_ep10_b9, label='b9')\n",
    "plt.legend(handles=[graph1, graph2, graph3],loc =\"lower right\")\n",
    "#plt.legend([\"blue\", \"green\",\"red\"], loc =\"lower right\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "celbeBYCsQ_Y"
   },
   "source": [
    "plt.plot(x, acc_cnn_noniid_r10_ep10_wb, label='2NN, $m=10$, $E=1$')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "liUJxZg7_VwT"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
