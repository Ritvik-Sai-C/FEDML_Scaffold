{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "xdzjRF_p7rhl"
   },
   "outputs": [],
   "source": [
    "#import necessary libraries for the code\n",
    "import numpy as np  #used mainly to save and load data from .npz files (our data in the drive is also stored in this format)\n",
    "import matplotlib.pyplot as plt #used for plotting or data visualization. In this code the part is all commented (the last 3-4 cells)\n",
    "import copy #used to make deepcopy of a neural network (meaning of deepcopy is explained in the cell below)\n",
    "import IPython  #IPython is an interactive command-line terminal for Python (but it is not used in any of the running code)\n",
    "from PIL import Image #used to perform tasks with images (but it is not used in running code)\n",
    "\n",
    "#every library below is used to tp create, train and evaluate the neural networks we create for our data\n",
    "#the use of each library is explained when it is used in the code\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "fD3qJKzDA3fc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nDifference between copy and deepcopy:\\nCopy/Shallow copy - creating a new pointer but area of data is the same\\ny = copy.copy(x) which visually means y -> □ <- x (both x and y are pointing to the same data block)\\nHence, x = 5 changes the value of y as well to 5\\n\\nDeepcopy - creating a new block of data with new pointer but same information\\ny = copy.deepcopy(x) which visually means x -> □ and y -> □ (separate copy of data is created)\\nHence, x = 5 doesn't affect the data in y\\n\""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Difference between copy and deepcopy:\n",
    "Copy/Shallow copy - creating a new pointer but area of data is the same\n",
    "y = copy.copy(x) which visually means y -> □ <- x (both x and y are pointing to the same data block)\n",
    "Hence, x = 5 changes the value of y as well to 5\n",
    "\n",
    "Deepcopy - creating a new block of data with new pointer but same information\n",
    "y = copy.deepcopy(x) which visually means x -> □ and y -> □ (separate copy of data is created)\n",
    "Hence, x = 5 doesn't affect the data in y\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EazUlvA2TXHF",
    "outputId": "1cf7f76a-a995-4454-c834-1fa4b23a71b6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# set random seeds (this is used to ensure that you get the same randomness everytime no matter how many times you run the code)\n",
    "np.random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "\n",
    "# set device (if gpu/cuda is available perform the neural network operations using that else use a cpu)\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(\"| using device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "VESLoY55Tag-"
   },
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "bsz = 10  #batch size\n",
    "no_clients = 10 #no.of clients\n",
    "lamda = 0.7 #not used anywhere in the code\n",
    "epsilon = 1e-10 #used in scaffold_experiment function (not sure what formula is used)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fBs3X1wdQtnq",
    "outputId": "e843fbe4-5d3c-4da0-bef1-7effaed31d3d"
   },
   "outputs": [],
   "source": [
    "#mounting drive to fetch noniid data from drive location\n",
    "#to make this work, first upload the data folder into your drive (only then data can be accessed)\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive',force_remount=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "A-BmfL-mhirk"
   },
   "outputs": [],
   "source": [
    "#a class NonIIDMyDataset is created to access and transform data\n",
    "class NonIIDMyDataset(Dataset):\n",
    "    #the __init__ function in Python is like the C++ constructor in an object-oriented approach\n",
    "    def __init__(self, data, targets, transform=None):\n",
    "        self.data = data  #data is X\n",
    "        self.targets = torch.LongTensor(targets)  #tragets are y. Convert y to a tensor to be able to used torch function on them\n",
    "        self.transform = transform  #this is the transformation to be applied on to X. By default the value is None.\n",
    "                                    #In the 2nd cell below you can see the exact transform used in the code\n",
    "\n",
    "    #this function is used to apply a transformation (if any) to X and return the pair (X, y) based on the index passed\n",
    "    def __getitem__(self, index):\n",
    "        x = self.data[index]\n",
    "        y = self.targets[index]\n",
    "\n",
    "        if self.transform:\n",
    "            # x = Image.fromarray(self.data[index].astype(np.uint8).transpose(1,2,0))\n",
    "            x = self.transform(x)\n",
    "\n",
    "        return x, y\n",
    "\n",
    "    #this function is used to get the length/no.of features of X\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "#The functions with __ at the front and back of every function in Python are called Magic Methods/Dunder Methods.\n",
    "#Magic methods are not meant to be invoked directly by you, but the invocation happens internally from the class on a certain action.\n",
    "#Not sure of the meaning but may be this concept is understood better if you find where these methods are used int he code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "WRYOEMIuNpBe"
   },
   "outputs": [],
   "source": [
    "#these are the locations of train and test data for 20 clients used in the code\n",
    "#there are many other folders as well in this dataset folder. May be different ones are used for different cases\n",
    "train_location = '/u/student/2020/cs20btech11046/resnet/old/dataset/practical/20/train/'\n",
    "test_location = '/u/student/2020/cs20btech11046/resnet/old/dataset/practical/20/test/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "stJUm9zgevMl"
   },
   "outputs": [],
   "source": [
    "#the transforms library imported above, is used to create a transformation of the data(X)\n",
    "#transforms.Compose - to put more than one sequantial transforms into one\n",
    "#transforms.ToTensor - to convert a list/np array to a tensor\n",
    "#transform.Normalize - transforms.Normalize(mean, std, inplace=False) to normalize a tensor with give mean and std\n",
    "#to normalize a data means changinf x to (x-mean)/std\n",
    "#here mean is 0.137 and std is 0.3081. May be these values are obtained by calculating mean and std of X separately or they are random. Not sure\n",
    "\n",
    "#how did these value we got\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "])\n",
    "\n",
    "client_loader = []  #this list is used to store the train data loaded using 'DataLoader' module from torch in batches\n",
    "\n",
    "#this function converts y of data to a tensor using __init__, applies the above transformation to x using the __getitenm__ function in the NonIIDMyDataset\n",
    "#and loads the data in batches (in order to train it with a neural network later) and stores the loaded data into client_loader list created above\n",
    "def noniid_train_loader(bsz=10):\n",
    "  #for all the 20 clients\n",
    "  for i in range(no_clients):\n",
    "    #go to the folder /content/drive/MyDrive/dataset/practical/20/train/, read the file from client_num.npz (liek 1.npz, 2.npz ... 20.npz) and store the X and y values\n",
    "    file_path = str(i)+'.npz'\n",
    "    loc = train_location + file_path\n",
    "    data = np.load(loc)\n",
    "    X = list(data['features'])\n",
    "    Y = list(data['labels'])\n",
    "\n",
    "    #create an object called dataset which is an instance of the class NonIIDMyDataset\n",
    "    dataset = NonIIDMyDataset(X, Y, transform=transform)\n",
    "    #in batches of 10, load the whole dataset and store it in client_load\n",
    "    client_load = torch.utils.data.DataLoader(dataset, batch_size=bsz, shuffle=True)\n",
    "\n",
    "    #append every client's dataload into client_loader list\n",
    "    client_loader.append(client_load)\n",
    "\n",
    "  print(client_loader)  #you can see 20 objects of torch dataloaders\n",
    "  return client_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zehfiWjBfCeA",
    "outputId": "d7fc28de-a0ba-4828-9231-3a7fde909831"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<torch.utils.data.dataloader.DataLoader object at 0x7f9950e62a50>, <torch.utils.data.dataloader.DataLoader object at 0x7f9950e4ec50>, <torch.utils.data.dataloader.DataLoader object at 0x7f9950e76a50>, <torch.utils.data.dataloader.DataLoader object at 0x7f9950e76c10>, <torch.utils.data.dataloader.DataLoader object at 0x7f9950e76e90>, <torch.utils.data.dataloader.DataLoader object at 0x7f9950e77110>, <torch.utils.data.dataloader.DataLoader object at 0x7f9950e61910>, <torch.utils.data.dataloader.DataLoader object at 0x7f9950e62710>, <torch.utils.data.dataloader.DataLoader object at 0x7f99511372d0>, <torch.utils.data.dataloader.DataLoader object at 0x7f9950e625d0>]\n"
     ]
    }
   ],
   "source": [
    "noniid_client_train_loader = noniid_train_loader(bsz = bsz) #call the above funtion to perform all the actions explained inside the func, noniid_train_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "N628K2e5M8HK"
   },
   "outputs": [],
   "source": [
    "#the exact same thing as in the func noniid_train_loader is done here. Expect that the data is extracted now read from the loacation /content/drive/MyDrive/dataset/practical/20/test\n",
    "test_loader = []\n",
    "def noniid_test_loader(batch_size,shuffle):\n",
    "  for i in range(no_clients):\n",
    "    file_path = str(i)+'.npz'\n",
    "    loc = test_location + file_path\n",
    "    data = np.load(loc)\n",
    "    X = list(data['features'])\n",
    "    Y = list(data['labels'])\n",
    "\n",
    "    dataset = NonIIDMyDataset(X, Y, transform=transform)\n",
    "    client_load = torch.utils.data.DataLoader(dataset, batch_size=bsz, shuffle=True)\n",
    "\n",
    "    test_loader.append(client_load)\n",
    "\n",
    "  print(test_loader)\n",
    "  return test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uij69lidL_1b",
    "outputId": "8c86f2ac-b7d3-4b56-e468-9bcbe74e1d75"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<torch.utils.data.dataloader.DataLoader object at 0x7f9949ed5b90>, <torch.utils.data.dataloader.DataLoader object at 0x7f9949ed5a50>, <torch.utils.data.dataloader.DataLoader object at 0x7f9949ed43d0>, <torch.utils.data.dataloader.DataLoader object at 0x7f9949ed5fd0>, <torch.utils.data.dataloader.DataLoader object at 0x7f9949ed6410>, <torch.utils.data.dataloader.DataLoader object at 0x7f9949ed6610>, <torch.utils.data.dataloader.DataLoader object at 0x7f9949ed6810>, <torch.utils.data.dataloader.DataLoader object at 0x7f9949ed69d0>, <torch.utils.data.dataloader.DataLoader object at 0x7f9949ed6b90>, <torch.utils.data.dataloader.DataLoader object at 0x7f9949ed6d50>]\n"
     ]
    }
   ],
   "source": [
    "test_loader = noniid_test_loader(batch_size = 1000, shuffle=False)  #test data is tranformed loaded in batches of 1000 and stored in test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uvdAGMQpfKj1",
    "outputId": "5b513ea5-1c45-412a-d011-db15a59f71d4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.utils.data.dataloader.DataLoader'>\n",
      "<torch.utils.data.dataloader.DataLoader object at 0x7f9950e62a50>\n",
      "non-iid:  tensor([101., 662.,   0.,   0.,   1., 245.,   0.,  23., 796., 144.])\n",
      "non-iid:  tensor([ 333.,  663.,    0.,    0., 4344.,  245.,    0.,   23.,  796.,  145.])\n"
     ]
    }
   ],
   "source": [
    "# non-iid\n",
    "#this cell is totally just for observation\n",
    "label_dist = torch.zeros(10)  #since we have 10 classes, create a torch array with 10 zeros\n",
    "print(type(noniid_client_train_loader[0]))\n",
    "print(noniid_client_train_loader[0])\n",
    "\n",
    "#using a for-loop, count the no.of rows is dataset which has 10 classes respectively for client 1\n",
    "for (x,y) in noniid_client_train_loader[0]:\n",
    "    label_dist+= torch.sum(F.one_hot(y,num_classes=10), dim=0)  #one-hot encoding is explained int he next cell\n",
    "\n",
    "print(\"non-iid: \", label_dist)\n",
    "#I suppose there should be a line like label_dist = torch.zeros(10) here as well\n",
    "for (x,y) in test_loader[0]:\n",
    "\n",
    "    label_dist+= torch.sum(F.one_hot(y,num_classes=10), dim=0)\n",
    "\n",
    "print(\"non-iid: \", label_dist)\n",
    "# fig, axes = plt.subplots(1, 2, figsize=(10, 4))\n",
    "# for i, ax in enumerate(axes.flat):\n",
    "#     ax.axis(\"off\")\n",
    "#     ax.set_title(label[i].cpu().numpy())\n",
    "#     ax.imshow(img[i][0], cmap=\"gray\")\n",
    "# IPython.display.display(fig)\n",
    "# plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "IlmXWOTb0c7u"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\none hot encoding is a concept where we assign 1 for the class of that row and 0 for the rest\\nexample say we have 5 classes in the dataset.\\nThe classes of say 10 rows of data are 1 3 2 4 1 5 3 2 1 4. (i.e., 1st row of data belongs to class 1 ...)\\nAfter applying one hot encoding the classes of these 10 rows will be represented as\\n1th row : 1 0 0 0 0\\n2th row : 0 0 1 0 0\\n3th row : 0 1 0 0 0\\n4th row : 0 0 0 1 0\\n5th row : 1 0 0 0 0\\n6th row : 0 0 0 0 1\\n7th row : 0 0 1 0 0\\n8th row : 0 1 0 0 0\\n9th row : 1 0 0 0 0\\n10th row: 0 0 0 1 0\\n'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "one hot encoding is a concept where we assign 1 for the class of that row and 0 for the rest\n",
    "example say we have 5 classes in the dataset.\n",
    "The classes of say 10 rows of data are 1 3 2 4 1 5 3 2 1 4. (i.e., 1st row of data belongs to class 1 ...)\n",
    "After applying one hot encoding the classes of these 10 rows will be represented as\n",
    "1th row : 1 0 0 0 0\n",
    "2th row : 0 0 1 0 0\n",
    "3th row : 0 1 0 0 0\n",
    "4th row : 0 0 0 1 0\n",
    "5th row : 1 0 0 0 0\n",
    "6th row : 0 0 0 0 1\n",
    "7th row : 0 0 1 0 0\n",
    "8th row : 0 1 0 0 0\n",
    "9th row : 1 0 0 0 0\n",
    "10th row: 0 0 0 1 0\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "LPO0winNG0Vn"
   },
   "outputs": [],
   "source": [
    "#this function is only used to observe how many parameters are used in the neural network we create. It is only for observation. Not to effect the running of any code\n",
    "#parameters in neural networks are like no.of weights or bias params included to the network. Check it out on the internet\n",
    "def num_params(model):\n",
    "    \"\"\" \"\"\"\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Kci2Pvx6kExd",
    "outputId": "df79c56c-1da8-4bef-990b-e4bca95ef123"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP(\n",
      "  (fc1): Linear(in_features=784, out_features=200, bias=True)\n",
      "  (fc2): Linear(in_features=200, out_features=200, bias=True)\n",
      "  (out): Linear(in_features=200, out_features=10, bias=True)\n",
      ")\n",
      "199210\n"
     ]
    }
   ],
   "source": [
    "# define fully connected NN\n",
    "#this version of Multi Layer Perceptron is not to train data in the code. Instead, the CNN defined in the next cell is used\n",
    "#the network here is: x -> linear layer -> relu activation -> linear layer -> relu activation -> linear -> output\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(784, 200);\n",
    "        self.fc2 = nn.Linear(200, 200);\n",
    "        self.out = nn.Linear(200, 10);\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.flatten(1) # [B x 784]\n",
    "        x = F.relu(self.fc1(x)) # [B x 200]\n",
    "        x = F.relu(self.fc2(x)) # [B x 200]\n",
    "        x = self.out(x) # [B x 10]\n",
    "        return x\n",
    "\n",
    "print(MLP())\n",
    "print(num_params(MLP()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "q6THHwG8k8tQ",
    "outputId": "24fc07ed-44f7-4c66-f666-11dcf2833918"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN(\n",
      "  (conv1): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (fc): Linear(in_features=1024, out_features=512, bias=True)\n",
      "  (out): Linear(in_features=512, out_features=10, bias=True)\n",
      ")\n",
      "582026\n"
     ]
    }
   ],
   "source": [
    "# define cnn\n",
    "#A CNN (Convolutional Neural Network) is another kind of NN.\n",
    "#In MLPs, there are layers like linear layers where a linear operation like y = w.T*x+b is applied (a linear operation) followed by activation\n",
    "#Similarly, in CNN, as the name suggests, convolution is done on x (input) to get y (output) on some layers. Here kernels are used.\n",
    "#I suggest you to look through some blogs and understand practically and mathematically\n",
    "\n",
    "#the network below is: input -> convolution 2D layer -> max pool activation -> conv 2d -> max pool -> linear -> relu -> linear -> output\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, 5)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 5)\n",
    "        self.fc = nn.Linear(1024, 512)\n",
    "        self.out = nn.Linear(512, 10)\n",
    "        self.loss = 0\n",
    "        self.control = {}\n",
    "        self.delta_control = {}\n",
    "        self.delta_y = {}\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.max_pool2d(self.conv1(x), 2, 2) # [B x 32 x 12 x 12]\n",
    "        x = F.max_pool2d(self.conv2(x), 2, 2) # [B x 64 x 4 x 4]\n",
    "        x = x.flatten(1) # [B x 1024]\n",
    "        x = F.relu(self.fc(x)) # [B x 512]\n",
    "        x = self.out(x) # [B x 10]\n",
    "        return x\n",
    "\n",
    "print(CNN())\n",
    "print(num_params(CNN()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "5bMA-Huh4RGO"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nThe whole idea of neural network and data revolves around the below steps:\\n1. Create a basic neural network be it MLP, CNN, RNN\\n2. Transform & Normalize data to be able to train and validate data using the network\\n3. Change the weights etc., parameters of the neural network through back propogation or any other method\\n4. For the same choose a loss function and an optimizer.\\n5. Repeat until you reach some fixed no.of iterations or desired result\\n\\nSo basically train your network with initial weights and the data and predict ŷ.\\nCalculate loss/error using the loss func you choose. An example is (y-ŷ).\\nIf the error is more, re-train the network with new weights. This is done through back propgation which is automatically done most of the times.\\n'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "The whole idea of neural network and data revolves around the below steps:\n",
    "1. Create a basic neural network be it MLP, CNN, RNN\n",
    "2. Transform & Normalize data to be able to train and validate data using the network\n",
    "3. Change the weights etc., parameters of the neural network through back propogation or any other method\n",
    "4. For the same choose a loss function and an optimizer.\n",
    "5. Repeat until you reach some fixed no.of iterations or desired result\n",
    "\n",
    "So basically train your network with initial weights and the data and predict ŷ.\n",
    "Calculate loss/error using the loss func you choose. An example is (y-ŷ).\n",
    "If the error is more, re-train the network with new weights. This is done through back propgation which is automatically done most of the times.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "pkcLDjnalBgd"
   },
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss() #the loss function we chose is cross entropy. The mathematical formula is available on the internet\n",
    "\n",
    "#the below function is used to validate (find the percentage of correctly predicted output)\n",
    "def validate(model,client):\n",
    "    #change the model/network to evaluation mode and for the given client, predict ŷ = model(x). If ŷ=y, add 1 to correct\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for (t, (x,y)) in enumerate(test_loader[client]):\n",
    "            x = x.to(device)\n",
    "            x = x.permute(0, 2, 3, 1)\n",
    "            #print(\"x\",x.shape)\n",
    "            y = y.to(device)\n",
    "            out = model(x)\n",
    "            correct += torch.sum(torch.argmax(out, dim=1) == y).item()\n",
    "            total += x.shape[0]\n",
    "    return correct/total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "WBF3a_JRlKur"
   },
   "outputs": [],
   "source": [
    "def train_client(id, client_loader, global_model, num_local_epochs, lr):\n",
    "    #create a deepcopy of the global model and change the network to train mode\n",
    "    local_model = copy.deepcopy(global_model)\n",
    "    local_model = local_model.to(device)\n",
    "    local_model.train()\n",
    "    optimizer = torch.optim.SGD(local_model.parameters(), lr=lr)\n",
    "\n",
    "    #for given no.of epochs (iterations) run the for-loop on the client\n",
    "    for epoch in range(num_local_epochs):\n",
    "        #for every pair of (X, y), predict ŷ for X using the local model, find the loss (using cross entropy loss here) which is l\n",
    "        #predict ŷ for X using the global model, find the loss with this ŷ and y which is dl\n",
    "        #I guess this is some new formula in this version. find the total loss for present (X, y) using the formula\n",
    "        #(1-p)*loss + p*drift_loss where p = l/(l+dl+epsilon)\n",
    "        #with this loss, as said in the 2nd cell above, we perform back propogation and optimize.\n",
    "        #I am exactly not sure of the math or steps which happen in back prop and optimization\n",
    "        #You can check it online. You may get an idea\n",
    "        for (i, (x,y)) in enumerate(client_loader):\n",
    "            x = x.to(device)\n",
    "\n",
    "            #x.reshape(10, 1, 28, 28)\n",
    "            x = x.permute(0, 2, 3, 1)\n",
    "            #print(\"x\",x.shape)\n",
    "            y = y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            out = local_model(x)\n",
    "            loss = criterion(out, y)\n",
    "#             global_output_batch = global_model(x)\n",
    "#             drift_loss = criterion(global_output_batch,y)\n",
    "            l = loss.item()#print('loss',loss.item())\n",
    "#             dl = drift_loss.item()\n",
    "            #print('drift loss',drift_loss.item())\n",
    "#             p = l/(l+dl+ epsilon)\n",
    "            #print(p)\n",
    "#             total_loss = (1-p)* loss+ p*drift_loss\n",
    "#             total_loss.backward()\n",
    "        \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    #after training the local model with the client's data for given no.of epochs, we return the local model\n",
    "    #So here, the overall model (like the server model) as effects the weights/params of local model\n",
    "    #cx every client has their own global model and local model like in our prev sem code\n",
    "    #remember we used to approach to remove the usage of server completely?\n",
    "    return local_model\n",
    "\n",
    "#this func is same as aggregation func in the prev sems code. models argument is an array of local models of all the clients\n",
    "def running_model_avg(models,client_no):\n",
    "    # Initialize the aggregated model's state dictionary\n",
    "    aggregated_state_dict = models[client_no].state_dict()\n",
    "    it = 0\n",
    "    # Iterate over the models of the clients\n",
    "    for client_model in models:\n",
    "        #if the clinet_model is not the one passed as an argument\n",
    "        if it!=client_no:\n",
    "          # Get the state dictionary of the current client model\n",
    "          client_state_dict = client_model.state_dict()\n",
    "\n",
    "          # Iterate over the parameters in the state dictionary\n",
    "          for param_name, param in client_state_dict.items():\n",
    "              # Perform aggregation for each parameter\n",
    "              if param_name.endswith(\".weight\") or param_name.endswith(\".bias\"):\n",
    "                  # Update the aggregated parameter by averaging\n",
    "                  aggregated_state_dict[param_name] += param\n",
    "        it = it + 1\n",
    "\n",
    "    # Compute the average by dividing by the number of client models\n",
    "    num_client_models = len(models)\n",
    "    for param_name in aggregated_state_dict:\n",
    "        aggregated_state_dict[param_name] /= num_client_models\n",
    "\n",
    "    # Create a new model instance for the aggregated model\n",
    "    aggregated_model = type(models[0])()\n",
    "    aggregated_model.load_state_dict(aggregated_state_dict)\n",
    "\n",
    "    return aggregated_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "_fv2VaSF9X5Q"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nBasically what is happening above is that, create a state_dict called aggregated_state_dict and intitialize it with our present client's data\\nNow run a for loop through all the other client's model and add their params (state_dict) to the aggregated_state_dict\\nThen to normalize it, divide all the params of aggreated_state_dict by no.of clients (20 here)\\nThen create a model structure for aggregated_model and load all these params into this.\\n\\nBut ig we can just add the params of all clients directly instead of that if it!=client_no statement. Cz at the end I feel we are just adding the params of all the local models.\\n\""
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Basically what is happening above is that, create a state_dict called aggregated_state_dict and intitialize it with our present client's data\n",
    "Now run a for loop through all the other client's model and add their params (state_dict) to the aggregated_state_dict\n",
    "Then to normalize it, divide all the params of aggreated_state_dict by no.of clients (20 here)\n",
    "Then create a model structure for aggregated_model and load all these params into this.\n",
    "\n",
    "But ig we can just add the params of all clients directly instead of that if it!=client_no statement. Cz at the end I feel we are just adding the params of all the local models.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "Cw0lZVFflOSL"
   },
   "outputs": [],
   "source": [
    "#modified scaffold each client having their own global model global_model[0] for client[0]\n",
    "\n",
    "def scaffold_experiment(global_model, num_clients, num_local_epochs, lr, client_train_loader, max_rounds, filename):   ##num_client\n",
    "    round_accuracy = []\n",
    "    #for all the rounds\n",
    "    for t in range(max_rounds):\n",
    "        print(\"starting round {}\".format(t))\n",
    "\n",
    "        # choose clients\n",
    "        #clients = np.random.choice(np.arange(100), num_clients, replace = False)  ###remove this\n",
    "        clients=num_clients\n",
    "        print(\"clients: \", clients)   ##\n",
    "        #create 2 list to store global (aggregated) and local models of each client\n",
    "        running_avg = [None for _ in range(clients)]\n",
    "        local_models = [None for _ in range(clients)]\n",
    "\n",
    "        for i in range(clients):\n",
    "          global_model[i].eval()\n",
    "          global_model[i] = global_model[i].to(device)\n",
    "          #running_avg = np.empty(clients, dtype='collections.OrderedDict') #None\n",
    "\n",
    "        #for all the clients, train their local models with their dataset\n",
    "        for i in range(clients):\n",
    "            # train local client\n",
    "            print(\"round {}, starting client {}/{}, id: {}\".format(t, i+1,num_clients, i+1))\n",
    "            local_models[i] = train_client(i, client_train_loader[i], global_model[i], num_local_epochs, lr)\n",
    "\n",
    "        # add local model parameters to running average of each client\n",
    "        for j in range(clients):\n",
    "          #print(type(running_avg[j]), '   and ',type(local_model.state_dict()))\n",
    "          global_model[j] = running_model_avg(local_models,j)\n",
    "\n",
    "\n",
    "       # validate\n",
    "        acc_val = 0\n",
    "        for client in range(no_clients):\n",
    "          val_acc = validate(global_model[client],client)\n",
    "          #print('each client',val_acc)\n",
    "          acc_val = acc_val + val_acc\n",
    "        acc_val = acc_val/no_clients\n",
    "        print(\"round {}, validation acc: {}\".format(t, val_acc))\n",
    "        round_accuracy.append(acc_val)\n",
    "\n",
    "        if (t % 10 == 0):\n",
    "          np.save(filename+'_{}'.format(t)+'.npy', np.array(round_accuracy))\n",
    "\n",
    "    return np.array(round_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Kj2J-t3vlUPX",
    "outputId": "8b932176-23a2-4ba7-a360-5378f82ece90"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN(\n",
      "  (conv1): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (fc): Linear(in_features=1024, out_features=512, bias=True)\n",
      "  (out): Linear(in_features=512, out_features=10, bias=True)\n",
      ")\n",
      "total params:  582026\n",
      "starting round 0\n",
      "clients:  10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/u/student/2020/cs20btech11046/anaconda3/lib/python3.11/site-packages/torch/cuda/__init__.py:611: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "round 0, starting client 1/10, id: 1\n",
      "round 0, starting client 2/10, id: 2\n",
      "round 0, starting client 3/10, id: 3\n",
      "round 0, starting client 4/10, id: 4\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 17\u001b[0m\n\u001b[1;32m     11\u001b[0m   cnn_noniid_r10_ep10[i] \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mdeepcopy(cnn)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m#cnn_iid_m10 = copy.deepcopy(cnn)\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m#calculate the accuracies obtained by the final aggregated global model for all the client's datasets\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m#we are passing, global models of all clients, no.of clients, no.of epochs, learning rate for optimizer,\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m#the train data of all clients loaded (at the beginning of this notebook), no.of rounds to run the exp, filepath to store the results\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m acc_cnn_noniid_r10_ep10 \u001b[38;5;241m=\u001b[39m scaffold_experiment(cnn_noniid_r10_ep10, num_clients\u001b[38;5;241m=\u001b[39mno_clients,\n\u001b[1;32m     18\u001b[0m                                  num_local_epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m,\n\u001b[1;32m     19\u001b[0m                                  lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.01\u001b[39m,\n\u001b[1;32m     20\u001b[0m                                  client_train_loader \u001b[38;5;241m=\u001b[39m noniid_client_train_loader,\n\u001b[1;32m     21\u001b[0m                                  max_rounds\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m,  \u001b[38;5;66;03m## for testing keep it 1\u001b[39;00m\n\u001b[1;32m     22\u001b[0m                                  filename\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/u/student/2020/cs20btech11046/resnet/old/dataset/results/acc_cnn_noniid_clt20_e20_biasvar\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28mprint\u001b[39m(acc_cnn_noniid_r10_ep10)\n\u001b[1;32m     24\u001b[0m np\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/u/student/2020/cs20btech11046/resnet/old/dataset/results/acc_cnn_noniid_clt20_ep20_biasvar.npy\u001b[39m\u001b[38;5;124m'\u001b[39m, acc_cnn_noniid_r10_ep10)\n",
      "Cell \u001b[0;32mIn[21], line 26\u001b[0m, in \u001b[0;36mscaffold_experiment\u001b[0;34m(global_model, num_clients, num_local_epochs, lr, client_train_loader, max_rounds, filename)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(clients):\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;66;03m# train local client\u001b[39;00m\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mround \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, starting client \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, id: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(t, i\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m,num_clients, i\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m---> 26\u001b[0m     local_models[i] \u001b[38;5;241m=\u001b[39m train_client(i, client_train_loader[i], global_model[i], num_local_epochs, lr)\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# add local model parameters to running average of each client\u001b[39;00m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(clients):\n\u001b[1;32m     30\u001b[0m   \u001b[38;5;66;03m#print(type(running_avg[j]), '   and ',type(local_model.state_dict()))\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[19], line 38\u001b[0m, in \u001b[0;36mtrain_client\u001b[0;34m(id, client_loader, global_model, num_local_epochs, lr)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m#             dl = drift_loss.item()\u001b[39;00m\n\u001b[1;32m     31\u001b[0m             \u001b[38;5;66;03m#print('drift loss',drift_loss.item())\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m#             p = l/(l+dl+ epsilon)\u001b[39;00m\n\u001b[1;32m     33\u001b[0m             \u001b[38;5;66;03m#print(p)\u001b[39;00m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;66;03m#             total_loss = (1-p)* loss+ p*drift_loss\u001b[39;00m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;66;03m#             total_loss.backward()\u001b[39;00m\n\u001b[1;32m     37\u001b[0m             loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m---> 38\u001b[0m             optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     40\u001b[0m     \u001b[38;5;66;03m#after training the local model with the client's data for given no.of epochs, we return the local model\u001b[39;00m\n\u001b[1;32m     41\u001b[0m     \u001b[38;5;66;03m#So here, the overall model (like the server model) as effects the weights/params of local model\u001b[39;00m\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;66;03m#cx every client has their own global model and local model like in our prev sem code\u001b[39;00m\n\u001b[1;32m     43\u001b[0m     \u001b[38;5;66;03m#remember we used to approach to remove the usage of server completely?\u001b[39;00m\n\u001b[1;32m     44\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m local_model\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/optim/optimizer.py:373\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    368\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    369\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    370\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    371\u001b[0m             )\n\u001b[0;32m--> 373\u001b[0m out \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    374\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    376\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/optim/optimizer.py:76\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     74\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     75\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n\u001b[0;32m---> 76\u001b[0m     ret \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     78\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/optim/sgd.py:73\u001b[0m, in \u001b[0;36mSGD.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m     70\u001b[0m d_p_list \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     71\u001b[0m momentum_buffer_list \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m---> 73\u001b[0m has_sparse_grad \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_group(group, params_with_grad, d_p_list, momentum_buffer_list)\n\u001b[1;32m     75\u001b[0m sgd(params_with_grad,\n\u001b[1;32m     76\u001b[0m     d_p_list,\n\u001b[1;32m     77\u001b[0m     momentum_buffer_list,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     84\u001b[0m     has_sparse_grad\u001b[38;5;241m=\u001b[39mhas_sparse_grad,\n\u001b[1;32m     85\u001b[0m     foreach\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mforeach\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     87\u001b[0m \u001b[38;5;66;03m# update momentum_buffers in state\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/optim/sgd.py:46\u001b[0m, in \u001b[0;36mSGD._init_group\u001b[0;34m(self, group, params_with_grad, d_p_list, momentum_buffer_list)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m p\u001b[38;5;241m.\u001b[39mgrad\u001b[38;5;241m.\u001b[39mis_sparse:\n\u001b[1;32m     44\u001b[0m     has_sparse_grad \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m---> 46\u001b[0m state \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate[p]\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmomentum_buffer\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m state:\n\u001b[1;32m     48\u001b[0m     momentum_buffer_list\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/_tensor.py:1002\u001b[0m, in \u001b[0;36mTensor.__hash__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    992\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    993\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIterating over a tensor might cause the trace to be incorrect. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    994\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPassing a tensor of different shape won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt change the number of \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    998\u001b[0m             stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[1;32m    999\u001b[0m         )\n\u001b[1;32m   1000\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28miter\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munbind(\u001b[38;5;241m0\u001b[39m))\n\u001b[0;32m-> 1002\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__hash__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m   1003\u001b[0m     \u001b[38;5;66;03m# Do NOT handle __torch_function__ here as user's default\u001b[39;00m\n\u001b[1;32m   1004\u001b[0m     \u001b[38;5;66;03m# implementation that handle most functions will most likely do it wrong.\u001b[39;00m\n\u001b[1;32m   1005\u001b[0m     \u001b[38;5;66;03m# It can be easily overridden by defining this method on the user\u001b[39;00m\n\u001b[1;32m   1006\u001b[0m     \u001b[38;5;66;03m# subclass if needed.\u001b[39;00m\n\u001b[1;32m   1007\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mid\u001b[39m(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m   1009\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__dir__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#cnn modified cnn_iid_m10 for each client seperate\n",
    "cnn = CNN() #create a CNN object\n",
    "print(cnn)\n",
    "print(\"total params: \", num_params(cnn))\n",
    "# CNN - iid - m=10 experiment\n",
    "cnn_noniid_r10_ep10 = np.empty(no_clients, dtype=CNN) #create a CNN with uninitialized and random params (weights and bias) with size 20\n",
    "\n",
    "#for all the 20 clients, make a deepcopy of cnn created in the first line of this cell (initializing the empty array created above)\n",
    "#these will be the global models for all the 20 clients\n",
    "for i in range(no_clients):\n",
    "  cnn_noniid_r10_ep10[i] = copy.deepcopy(cnn)\n",
    "\n",
    "#cnn_iid_m10 = copy.deepcopy(cnn)\n",
    "#calculate the accuracies obtained by the final aggregated global model for all the client's datasets\n",
    "#we are passing, global models of all clients, no.of clients, no.of epochs, learning rate for optimizer,\n",
    "#the train data of all clients loaded (at the beginning of this notebook), no.of rounds to run the exp, filepath to store the results\n",
    "acc_cnn_noniid_r10_ep10 = scaffold_experiment(cnn_noniid_r10_ep10, num_clients=no_clients,\n",
    "                                 num_local_epochs=5,\n",
    "                                 lr=0.01,\n",
    "                                 client_train_loader = noniid_client_train_loader,\n",
    "                                 max_rounds=10,  ## for testing keep it 1\n",
    "                                 filename='/u/student/2020/cs20btech11046/resnet/old/dataset/results/acc_cnn_noniid_clt20_e20_biasvar')\n",
    "print(acc_cnn_noniid_r10_ep10)\n",
    "np.save('/u/student/2020/cs20btech11046/resnet/old/dataset/results/acc_cnn_noniid_clt20_ep20_biasvar.npy', acc_cnn_noniid_r10_ep10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gaAEiZsdTHwD"
   },
   "outputs": [],
   "source": [
    "#this function is not use anywhere in the code\n",
    "def view_10(img, label):\n",
    "    \"\"\" view 10 labelled examples from tensor\"\"\"\n",
    "    fig, axes = plt.subplots(2, 5, figsize=(10, 4))\n",
    "    for i, ax in enumerate(axes.flat):\n",
    "        ax.axis(\"off\")\n",
    "        ax.set_title(label[i].cpu().numpy())\n",
    "        ax.imshow(img[i][0], cmap=\"gray\")\n",
    "    IPython.display.display(fig)\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "smplc98wIWKv"
   },
   "outputs": [],
   "source": [
    " acc_cnn_noniid_r10_ep10 = np.load('/u/student/2020/cs20btech11046/resnet/old/dataset/results/acc_cnn_noniid_r10_ep10_bias9.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F8EdqdNVI5sV"
   },
   "outputs": [],
   "source": [
    "x = np.arange(0,15)\n",
    "plt.figure(figsize=(8,6))\n",
    "\n",
    "plt.title(\"Scaffold test accuracy after $t$ rounds on non-iid MNIST\")\n",
    "\n",
    "plt.xlabel(\"Communication rounds $t$\")\n",
    "plt.ylabel(\"Test accuracy\")\n",
    "plt.axis([0, 15, 0.3, 1])\n",
    "\n",
    "\n",
    "\n",
    "plt.axhline(y=0.7, color='r', linestyle='dashed')\n",
    "plt.axhline(y=0.9, color='b', linestyle='dashed')\n",
    "\n",
    "plt.plot(x, acc_cnn_noniid_r10_ep10, label='2NN, $m=10$, $E=1$')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eOSMoo4W6Y2c"
   },
   "outputs": [],
   "source": [
    "acc_cnn_noniid_r10_ep10_b = np.load('/u/student/2020/cs20btech11046/resnet/old/dataset/results/acc_cnn_noniid_r10_ep10_bias8.npy')\n",
    "acc_cnn_noniid_r10_ep10_wb = np.load('/u/student/2020/cs20btech11046/resnet/old/dataset/results/acc_cnn_noniid_r10_ep10_nobias.npy')\n",
    "acc_cnn_noniid_r10_ep10_b9 = np.load('/u/student/2020/cs20btech11046/resnet/old/dataset/results/acc_cnn_noniid_r10_ep10_biasvar.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6BEq4HJb6egL"
   },
   "outputs": [],
   "source": [
    "x = np.arange(0,15)\n",
    "plt.figure(figsize=(8,6))\n",
    "\n",
    "plt.title(\"Scaffold test accuracy after $t$ rounds on non-iid MNIST\")\n",
    "\n",
    "plt.xlabel(\"Communication rounds $t$\")\n",
    "plt.ylabel(\"Test accuracy\")\n",
    "plt.axis([0, 15, 0.3, 1])\n",
    "\n",
    "\n",
    "\n",
    "plt.axhline(y=0.5, color='r', linestyle='dashed')\n",
    "plt.axhline(y=0.9, color='b', linestyle='dashed')\n",
    "\n",
    "graph1, =plt.plot(x, acc_cnn_noniid_r10_ep10_b, label='b8')\n",
    "graph2, =plt.plot(x, acc_cnn_noniid_r10_ep10_wb, label='wb')\n",
    "graph3, =plt.plot(x, acc_cnn_noniid_r10_ep10_b9, label='b9')\n",
    "plt.legend(handles=[graph1, graph2, graph3],loc =\"lower right\")\n",
    "#plt.legend([\"blue\", \"green\",\"red\"], loc =\"lower right\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "celbeBYCsQ_Y"
   },
   "source": [
    "plt.plot(x, acc_cnn_noniid_r10_ep10_wb, label='2NN, $m=10$, $E=1$')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "liUJxZg7_VwT"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
