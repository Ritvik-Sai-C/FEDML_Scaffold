{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "xdzjRF_p7rhl"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['archive.zip', 'skin-cancer-mnist-ham10000']\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "# python libraties\n",
    "import os, cv2,itertools\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from glob import glob\n",
    "from PIL import Image\n",
    "import pickle\n",
    "\n",
    "\n",
    "# pytorch libraries\n",
    "import torch\n",
    "from torch import optim,nn\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader,Dataset\n",
    "from torchvision import models,transforms\n",
    "\n",
    "# sklearn libraries\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# to make the results are reproducible\n",
    "np.random.seed(10)\n",
    "torch.manual_seed(10)\n",
    "torch.cuda.manual_seed(10)\n",
    "\n",
    "print(os.listdir(\"/u/student/2020/cs20btech11046/resnet/input\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fD3qJKzDA3fc"
   },
   "source": [
    "'''\n",
    "Difference between copy and deepcopy:\n",
    "Copy/Shallow copy - creating a new pointer but area of data is the same\n",
    "y = copy.copy(x) which visually means y -> □ <- x (both x and y are pointing to the same data block)\n",
    "Hence, x = 5 changes the value of y as well to 5\n",
    "\n",
    "Deepcopy - creating a new block of data with new pointer but same information\n",
    "y = copy.deepcopy(x) which visually means x -> □ and y -> □ (separate copy of data is created)\n",
    "Hence, x = 5 doesn't affect the data in y\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EazUlvA2TXHF",
    "outputId": "1cf7f76a-a995-4454-c834-1fa4b23a71b6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# set random seeds (this is used to ensure that you get the same randomness everytime no matter how many times you run the code)\n",
    "np.random.seed(10)\n",
    "torch.manual_seed(10)\n",
    "torch.cuda.manual_seed(10)\n",
    "\n",
    "# set device (if gpu/cuda is available perform the neural network operations using that else use a cpu)\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(\"| using device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "VESLoY55Tag-"
   },
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "bsz = 10  #batch size\n",
    "no_clients = 10 #no.of clients\n",
    "lamda = 0.7 #not used anywhere in the code\n",
    "epsilon = 1e-10 #used in scaffold_experiment function (not sure what formula is used)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fBs3X1wdQtnq",
    "outputId": "e843fbe4-5d3c-4da0-bef1-7effaed31d3d"
   },
   "outputs": [],
   "source": [
    "#mounting drive to fetch noniid data from drive location\n",
    "#to make this work, first upload the data folder into your drive (only then data can be accessed)\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive',force_remount=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '/u/student/2020/cs20btech11046/resnet/input/skin-cancer-mnist-ham10000'\n",
    "all_image_path = glob(os.path.join(data_dir, '*', '*.jpg'))\n",
    "imageid_path_dict = {os.path.splitext(os.path.basename(x))[0]: x for x in all_image_path}\n",
    "lesion_type_dict = {\n",
    "    'nv': 'Melanocytic nevi',\n",
    "    'mel': 'dermatofibroma',\n",
    "    'bkl': 'Benign keratosis-like lesions ',\n",
    "    'bcc': 'Basal cell carcinoma',\n",
    "    'akiec': 'Actinic keratoses',\n",
    "    'vasc': 'Vascular lesions',\n",
    "    'df': 'Dermatofibroma'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_img_mean_std(image_paths):\n",
    "    \"\"\"\n",
    "        computing the mean and std of three channel on the whole dataset,\n",
    "        first we should normalize the image from 0-255 to 0-1\n",
    "    \"\"\"\n",
    "\n",
    "    img_h, img_w = 224, 224\n",
    "    imgs = []\n",
    "    means, stdevs = [], []\n",
    "\n",
    "    for i in tqdm(range(len(image_paths))):\n",
    "        img = cv2.imread(image_paths[i])\n",
    "        img = cv2.resize(img, (img_h, img_w))\n",
    "        imgs.append(img)\n",
    "\n",
    "    imgs = np.stack(imgs, axis=3)\n",
    "    print(imgs.shape)\n",
    "\n",
    "    imgs = imgs.astype(np.float32) / 255.\n",
    "\n",
    "    for i in range(3):\n",
    "        pixels = imgs[:, :, i, :].ravel()  # resize to one row\n",
    "        means.append(np.mean(pixels))\n",
    "        stdevs.append(np.std(pixels))\n",
    "\n",
    "    means.reverse()  # BGR --> RGB\n",
    "    stdevs.reverse()\n",
    "\n",
    "    print(\"normMean = {}\".format(means))\n",
    "    print(\"normStd = {}\".format(stdevs))\n",
    "    return means,stdevs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|█████████████████▋                   | 9586/20030 [00:59<01:05, 160.04it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m norm_mean,norm_std \u001b[38;5;241m=\u001b[39m compute_img_mean_std(all_image_path)\n",
      "Cell \u001b[0;32mIn[6], line 12\u001b[0m, in \u001b[0;36mcompute_img_mean_std\u001b[0;34m(image_paths)\u001b[0m\n\u001b[1;32m      9\u001b[0m means, stdevs \u001b[38;5;241m=\u001b[39m [], []\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(image_paths))):\n\u001b[0;32m---> 12\u001b[0m     img \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mimread(image_paths[i])\n\u001b[1;32m     13\u001b[0m     img \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mresize(img, (img_h, img_w))\n\u001b[1;32m     14\u001b[0m     imgs\u001b[38;5;241m.\u001b[39mappend(img)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "norm_mean,norm_std = compute_img_mean_std(all_image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_original = pd.read_csv(os.path.join(data_dir, 'HAM10000_metadata.csv'))\n",
    "df_original['path'] = df_original['image_id'].map(imageid_path_dict.get)\n",
    "df_original['cell_type'] = df_original['dx'].map(lesion_type_dict.get)\n",
    "df_original['cell_type_idx'] = pd.Categorical(df_original['cell_type']).codes\n",
    "df_original.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this will tell us how many images are associated with each lesion_id\n",
    "df_undup = df_original.groupby('lesion_id').count()\n",
    "# now we filter out lesion_id's that have only one image associated with it\n",
    "df_undup = df_undup[df_undup['image_id'] == 1]\n",
    "df_undup.reset_index(inplace=True)\n",
    "df_undup.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here we identify lesion_id's that have duplicate images and those that have only one image.\n",
    "def get_duplicates(x):\n",
    "    unique_list = list(df_undup['lesion_id'])\n",
    "    if x in unique_list:\n",
    "        return 'unduplicated'\n",
    "    else:\n",
    "        return 'duplicated'\n",
    "\n",
    "# create a new colum that is a copy of the lesion_id column\n",
    "df_original['duplicates'] = df_original['lesion_id']\n",
    "# apply the function to this new column\n",
    "df_original['duplicates'] = df_original['duplicates'].apply(get_duplicates)\n",
    "df_original.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_original['duplicates'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we filter out images that don't have duplicates\n",
    "df_undup = df_original[df_original['duplicates'] == 'unduplicated']\n",
    "df_undup.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we create a val set using df because we are sure that none of these images have augmented duplicates in the train set\n",
    "y = df_undup['cell_type_idx']\n",
    "_, df_val = train_test_split(df_undup, test_size=0.2, random_state=101, stratify=y)\n",
    "df_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_val['cell_type_idx'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This set will be df_original excluding all rows that are in the val set\n",
    "# This function identifies if an image is part of the train or val set.\n",
    "def get_val_rows(x):\n",
    "    # create a list of all the lesion_id's in the val set\n",
    "    val_list = list(df_val['image_id'])\n",
    "    if str(x) in val_list:\n",
    "        return 'val'\n",
    "    else:\n",
    "        return 'train'\n",
    "\n",
    "# identify train and val rows\n",
    "# create a new colum that is a copy of the image_id column\n",
    "df_original['train_or_val'] = df_original['image_id']\n",
    "# apply the function to this new column\n",
    "df_original['train_or_val'] = df_original['train_or_val'].apply(get_val_rows)\n",
    "# filter out train rows\n",
    "df_train = df_original[df_original['train_or_val'] == 'train']\n",
    "print(len(df_train))\n",
    "print(len(df_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['cell_type'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_val['cell_type'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy fewer class to balance the number of 7 classes\n",
    "data_aug_rate = [15,10,5,50,0,40,5]\n",
    "for i in range(7):\n",
    "    if data_aug_rate[i]:\n",
    "     #   df_train=df_train.append([df_train.loc[df_train['cell_type_idx'] == i,:]]*(data_aug_rate[i]-1), ignore_index=True)\n",
    "      df_train = pd.concat([df_train] + [df_train.loc[df_train['cell_type_idx'] == i, :]] * (data_aug_rate[i] - 1), ignore_index=True)\n",
    "\n",
    "    #    df_train=df_train.append([df_train.loc[df_train['cell_type_idx'] == i,:]]*(data_aug_rate[i]-1), ignore_index=True)\n",
    "df_train['cell_type'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy fewer class to balance the number of 7 classes\n",
    "data_aug_rate = [15,10,5,50,0,40,5]\n",
    "for i in range(7):\n",
    "    if data_aug_rate[i]:\n",
    "     #   df_train=df_train.append([df_train.loc[df_train['cell_type_idx'] == i,:]]*(data_aug_rate[i]-1), ignore_index=True)\n",
    "      df_val = pd.concat([df_val] + [df_val.loc[df_val['cell_type_idx'] == i, :]] * (data_aug_rate[i] - 1), ignore_index=True)\n",
    "\n",
    "    #    df_train=df_train.append([df_train.loc[df_train['cell_type_idx'] == i,:]]*(data_aug_rate[i]-1), ignore_index=True)\n",
    "df_val['cell_type'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['cell_type'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # We can split the test set again in a validation set and a true test set:\n",
    "# df_val, df_test = train_test_split(df_val, test_size=0.5)\n",
    "df_train = df_train.reset_index()\n",
    "#df_val = df_val.reset_index()\n",
    "# df_test = df_test.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_splits = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the shuffled DataFrame into 10 equal parts\n",
    "df_shuffled = df_train.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "# df_shuffled = df_train\n",
    "\n",
    "\n",
    "split_size = len(df_train) // (num_splits)\n",
    "\n",
    "df_splits_train = []\n",
    "\n",
    "for i in range(num_splits):\n",
    "    start_idx = i * split_size\n",
    "    end_idx = (i + 1) * split_size\n",
    "    df_split = df_shuffled.iloc[start_idx:end_idx]\n",
    "    df_split = df_split.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "    df_splits_train.append(df_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_splits_train[1]['cell_type'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_shuffled = df_val.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "#df_shuffled = df_val\n",
    "\n",
    "split_size = len(df_val) // num_splits\n",
    "\n",
    "df_splits_val = []\n",
    "\n",
    "for i in range(num_splits):\n",
    "    start_idx = i * split_size\n",
    "    end_idx = (i + 1) * split_size\n",
    "    df_split = df_shuffled.iloc[start_idx:end_idx]\n",
    "    df_split = df_split.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "    df_splits_val.append(df_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_splits_val[1]['cell_type'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(num_splits):\n",
    "    np.savez('/u/student/2020/cs20btech11046/resnet/new/dataset/10/train/'+str(i), df_splits_train[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(num_splits):\n",
    "    np.savez('/u/student/2020/cs20btech11046/resnet/new/dataset/10/test/'+str(i), df_splits_val[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testaa =np.load('/u/student/2020/cs20btech11046/resnet/new/dataset/10/train/0.npz')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature_extract is a boolean that defines if we are finetuning or feature extracting. \n",
    "# If feature_extract = False, the model is finetuned and all model parameters are updated. \n",
    "# If feature_extract = True, only the last layer parameters are updated, the others remain fixed.\n",
    "def set_parameter_requires_grad(model, feature_extracting):\n",
    "    if feature_extracting:\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_model(model_name, num_classes, feature_extract, use_pretrained=True):\n",
    "    # Initialize these variables which will be set in this if statement. Each of these\n",
    "    #   variables is model specific.\n",
    "    model_ft = None\n",
    "    input_size = 0\n",
    "\n",
    "    if model_name == \"resnet\":\n",
    "        \"\"\" Resnet18, resnet34, resnet50, resnet101\n",
    "        \"\"\"\n",
    "        model_ft = models.resnet50(pretrained=use_pretrained)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        num_ftrs = model_ft.fc.in_features\n",
    "        model_ft.fc = nn.Linear(num_ftrs, num_classes)\n",
    "        input_size = 224\n",
    "\n",
    "\n",
    "    elif model_name == \"vgg\":\n",
    "        \"\"\" VGG11_bn\n",
    "        \"\"\"\n",
    "        model_ft = models.vgg11_bn(pretrained=use_pretrained)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        num_ftrs = model_ft.classifier[6].in_features\n",
    "        model_ft.classifier[6] = nn.Linear(num_ftrs,num_classes)\n",
    "        input_size = 224\n",
    "\n",
    "\n",
    "    elif model_name == \"densenet\":\n",
    "        \"\"\" Densenet121\n",
    "        \"\"\"\n",
    "        model_ft = models.densenet121(pretrained=use_pretrained)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        num_ftrs = model_ft.classifier.in_features\n",
    "        model_ft.classifier = nn.Linear(num_ftrs, num_classes)\n",
    "        input_size = 224\n",
    "\n",
    "    elif model_name == \"inception\":\n",
    "        \"\"\" Inception v3\n",
    "        Be careful, expects (299,299) sized images and has auxiliary output\n",
    "        \"\"\"\n",
    "        model_ft = models.inception_v3(pretrained=use_pretrained)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        # Handle the auxilary net\n",
    "        num_ftrs = model_ft.AuxLogits.fc.in_features\n",
    "        model_ft.AuxLogits.fc = nn.Linear(num_ftrs, num_classes)\n",
    "        # Handle the primary net\n",
    "        num_ftrs = model_ft.fc.in_features\n",
    "        model_ft.fc = nn.Linear(num_ftrs,num_classes)\n",
    "        input_size = 299\n",
    "\n",
    "    else:\n",
    "        print(\"Invalid model name, exiting...\")\n",
    "        exit()\n",
    "    return model_ft, input_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# resnet,vgg,densenet,inception\n",
    "model_name = 'densenet'\n",
    "num_classes = 7\n",
    "feature_extract = False\n",
    "# Initialize the model for this run\n",
    "model_ft, input_size = initialize_model(model_name, num_classes, feature_extract, use_pretrained=True)\n",
    "# Define the device:\n",
    "device = torch.device('cuda:0')\n",
    "# Put the model on the device:\n",
    "model = model_ft.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# norm_mean = (0.49139968, 0.48215827, 0.44653124)\n",
    "# norm_std = (0.24703233, 0.24348505, 0.26158768)\n",
    "# define the transformation of the train images.\n",
    "train_transform = transforms.Compose([transforms.Resize((input_size,input_size)),transforms.RandomHorizontalFlip(),\n",
    "                                      transforms.RandomVerticalFlip(),transforms.RandomRotation(20),\n",
    "                                      transforms.ColorJitter(brightness=0.1, contrast=0.1, hue=0.1),\n",
    "                                        transforms.ToTensor(), transforms.Normalize(norm_mean, norm_std)])\n",
    "# define the transformation of the val images.\n",
    "val_transform = transforms.Compose([transforms.Resize((input_size,input_size)), transforms.ToTensor(),\n",
    "                                    transforms.Normalize(norm_mean, norm_std)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a pytorch dataloader for this dataset\n",
    "class HAM10000(Dataset):\n",
    "    def __init__(self, df, transform=None):\n",
    "        self.df = df\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # Load data and get label\n",
    "        X = Image.open(self.df['path'][index])\n",
    "        y = torch.tensor(int(self.df['cell_type_idx'][index]))\n",
    "\n",
    "        if self.transform:\n",
    "            X = self.transform(X)\n",
    "\n",
    "        return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(df_train))\n",
    "print(len(df_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the training set using the table train_df and using our defined transitions (train_transform)\n",
    "training_sets, validation_sets,train_loaders,val_loaders= [],[],[],[]\n",
    "for i in range(num_splits):\n",
    "    training_set = HAM10000(df_splits_train[i], transform=train_transform)\n",
    "    training_sets.append(training_set)\n",
    "    train_loader = DataLoader(training_sets[i], batch_size=10, shuffle=False, num_workers=4)\n",
    "    train_loaders.append(train_loader ) \n",
    "    # Same for the validation set:\n",
    "    validation_set = HAM10000(df_splits_val[i], transform=train_transform)\n",
    "    validation_sets.append(validation_set)\n",
    "    val_loader = DataLoader(validation_sets[i], batch_size=10, shuffle=False, num_workers=4)\n",
    "    val_loaders.append(val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we use Adam optimizer, use cross entropy loss as our loss function\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "criterion = nn.CrossEntropyLoss().to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function is used during training process, to calculation the loss and accuracy\n",
    "class AverageMeter(object):\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_loss_train, total_acc_train = [],[]\n",
    "def train_client(id, train_loader, global_model, epoch,lr):\n",
    "    local_model = copy.deepcopy(global_model)\n",
    "    local_model = local_model.to(device)\n",
    "    local_model.train()\n",
    "    train_loss = AverageMeter()\n",
    "    train_acc = AverageMeter()\n",
    "    curr_iter = (epoch - 1) * len(train_loader)\n",
    "    for epoch in range(epoch):\n",
    "        \n",
    "        for i, (images, labels) in enumerate(train_loader):\n",
    "            \n",
    "         #   print(i)\n",
    "            length = images.size(0)\n",
    "            # print('image shape:',images.size(0), 'label shape',labels.size(0))\n",
    "            images = Variable(images).to(device)\n",
    "            labels = Variable(labels).to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "\n",
    "            loss = criterion(outputs, labels)\n",
    "            global_output_batch = global_model(images)\n",
    "            drift_loss = criterion(global_output_batch,labels)\n",
    "            l = loss.item()#print('loss',loss.item())\n",
    "            dl = drift_loss.item()\n",
    "            #print('drift loss',drift_loss.item())\n",
    "            p = l/(l+dl+ epsilon)\n",
    "            #print(p)\n",
    "            total_loss = (1-p)* loss+ p*drift_loss\n",
    "            total_loss.backward()\n",
    "            optimizer.step()\n",
    "            ########################################\n",
    "            prediction = outputs.max(1, keepdim=True)[1]\n",
    "            train_acc.update(prediction.eq(labels.view_as(prediction)).sum().item()/length)\n",
    "            train_loss.update(loss.item())\n",
    "            curr_iter += 1\n",
    "            if (i + 1) % 100 == 0:\n",
    "                print('[epoch %d], [iter %d / %d], [train loss %.5f], [train acc %.5f]' % (\n",
    "                    epoch, i + 1, len(train_loader), train_loss.avg, train_acc.avg))\n",
    "                total_loss_train.append(train_loss.avg)\n",
    "                total_acc_train.append(train_acc.avg)\n",
    "        #return train_loss.avg, train_acc.avg\n",
    "        return local_model\n",
    "    ############################################\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "def running_model_avg(models,client_no):\n",
    "    # Initialize the aggregated model's state dictionary\n",
    "    aggregated_state_dict = models[client_no].state_dict()\n",
    "\n",
    "    \n",
    "    it = 0\n",
    "    # Iterate over the models of the clients\n",
    "    for client_model in models:\n",
    "        #if the clinet_model is not the one passed as an argument\n",
    "        if it!=client_no:\n",
    "          # Get the state dictionary of the current client model\n",
    "          client_state_dict = client_model.state_dict()\n",
    "\n",
    "          # Iterate over the parameters in the state dictionary\n",
    "          for param_name, param in client_state_dict.items():\n",
    "                print(param_name)\n",
    "              # Perform aggregation for each parameter\n",
    "                if param_name.endswith(\".weight\") or param_name.endswith(\".bias\"):\n",
    "                  # Update the aggregated parameter by averaging\n",
    "                    aggregated_state_dict[param_name] += param\n",
    "#                 print(param)\n",
    "        it = it + 1\n",
    "\n",
    "    # Compute the average by dividing by the number of client models\n",
    "    num_client_models = len(models)\n",
    "#    print(num_client_models)\n",
    "#     num_client_models = 10\n",
    "    \n",
    "    for param_name in aggregated_state_dict:\n",
    "#         print(param_name)\n",
    "#         print(aggregated_state_dict[param_name])\n",
    "        \n",
    "#         print(type(aggregated_state_dict[param_name]))\n",
    "        if param_name.endswith(\".weight\") or param_name.endswith(\".bias\"):\n",
    "            aggregated_state_dict[param_name] /= num_client_models\n",
    "\n",
    "    # Create a new model instance for the aggregated model\n",
    "    aggregated_model = type(models[0])()\n",
    "    aggregated_model.load_state_dict(aggregated_state_dict)\n",
    "\n",
    "    return aggregated_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(val_loader, model, epoch):\n",
    "    model.eval()\n",
    "    val_loss = AverageMeter()\n",
    "    val_acc = AverageMeter()\n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(val_loader):\n",
    "            images, labels = data\n",
    "            N = images.size(0)\n",
    "            images = Variable(images).to(device)\n",
    "            labels = Variable(labels).to(device)\n",
    "\n",
    "            outputs = model(images)\n",
    "            prediction = outputs.max(1, keepdim=True)[1]\n",
    "\n",
    "            val_acc.update(prediction.eq(labels.view_as(prediction)).sum().item()/N)\n",
    "\n",
    "            val_loss.update(criterion(outputs, labels).item())\n",
    "\n",
    "    print('------------------------------------------------------------')\n",
    "    print('[client %d], [val loss %.5f], [val acc %.5f]' % (epoch, val_loss.avg, val_acc.avg))\n",
    "    print('------------------------------------------------------------')\n",
    "    return val_loss.avg, val_acc.avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#modified scaffold each client having their own global model global_model[0] for client[0]\n",
    "\n",
    "def scaffold_experiment(global_model, num_clients, num_local_epochs, lr, client_train_loader, max_rounds, filename):   ##num_client\n",
    "    round_accuracy = []\n",
    "    #for all the rounds\n",
    "    for t in range(max_rounds):\n",
    "        print(\"starting round {}\".format(t))\n",
    "\n",
    "        # choose clients\n",
    "        #clients = np.random.choice(np.arange(100), num_clients, replace = False)  ###remove this\n",
    "        clients=num_clients\n",
    "        print(\"clients: \", clients)   ##\n",
    "        #create 2 list to store global (aggregated) and local models of each client\n",
    "        running_avg = [None for _ in range(clients)]\n",
    "        local_models = [None for _ in range(clients)]\n",
    "\n",
    "        for i in range(clients):\n",
    "            global_model[i].eval()\n",
    "            global_model[i] = global_model[i].to(device)\n",
    "          #running_avg = np.empty(clients, dtype='collections.OrderedDict') #None\n",
    "\n",
    "        #for all the clients, train their local models with their dataset\n",
    "        for i in range(clients):\n",
    "            # train local client\n",
    "            print(\"round {}, starting client {}/{}, id: {}\".format(t, i+1,num_clients, i+1))\n",
    "            local_models[i] = train_client(i, train_loaders[i], global_model[i], num_local_epochs, lr)\n",
    "\n",
    "        # add local model parameters to running average of each client\n",
    "        for j in range(clients):\n",
    "          #print(type(running_avg[j]), '   and ',type(local_model.state_dict()))\n",
    "          global_model[j] = running_model_avg(local_models,j)\n",
    "\n",
    "\n",
    "       # validate\n",
    "        acc_val = 0\n",
    "        for client in range(no_clients):\n",
    "            val_loss, val_acc = validate(val_loaders[client],global_model[client],client)\n",
    "            #print('each client',val_acc)\n",
    "            acc_val = acc_val + val_acc\n",
    "        acc_val = acc_val/no_clients\n",
    "        print(\"round {}, validation acc: {}\".format(t, val_acc))\n",
    "        round_accuracy.append(acc_val)\n",
    "\n",
    "        if (t % 10 == 0):\n",
    "            np.save(filename+'_{}'.format(t)+'.npy', np.array(round_accuracy))\n",
    "\n",
    "    return np.array(round_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "model_name = 'densenet'\n",
    "num_classes = 7\n",
    "feature_extract = False\n",
    "# Initialize the model for this run\n",
    "model_ft, input_size = initialize_model(model_name, num_classes, feature_extract, use_pretrained=True)\n",
    "# Define the device:\n",
    "device = torch.device('cuda:0')\n",
    "# Put the model on the device:\n",
    "model = model_ft.to(device)\n",
    "\n",
    "\n",
    "#cnn modified cnn_iid_m10 for each client seperate\n",
    "# cnn = CNN() #create a CNN object\n",
    "# print(cnn)\n",
    "#print(\"total params: \", num_params(model))\n",
    "# CNN - iid - m=10 experiment\n",
    "#cnn_noniid_r10_ep10 = np.empty(no_clients, dtype= model) #create a CNN with uninitialized and random params (weights and bias) with size 20\n",
    "cnn_noniid_r10_ep10 = [copy.deepcopy(model) for _ in range(no_clients)]\n",
    "#for all the 20 clients, make a deepcopy of cnn created in the first line of this cell (initializing the empty array created above)\n",
    "#these will be the global models for all the 20 clients\n",
    "\n",
    "# for i in range(no_clients):\n",
    "#   cnn_noniid_r10_ep10[i] = copy.deepcopy(model)\n",
    "\n",
    "#cnn_iid_m10 = copy.deepcopy(cnn)\n",
    "#calculate the accuracies obtained by the final aggregated global model for all the client's datasets\n",
    "#we are passing, global models of all clients, no.of clients, no.of epochs, learning rate for optimizer,\n",
    "#the train data of all clients loaded (at the beginning of this notebook), no.of rounds to run the exp, filepath to store the results\n",
    "acc_cnn_noniid_r10_ep10 = scaffold_experiment(cnn_noniid_r10_ep10, num_clients=no_clients,\n",
    "                                 num_local_epochs=5,\n",
    "                                 lr=0.01,\n",
    "                                 client_train_loader = train_loaders,\n",
    "                                 max_rounds=10,  ## for testing keep it 1\n",
    "                                 filename='/u/student/2020/cs20btech11046/resnet/new/dataset/results/acc_cnn_noniid_clt20_e20_biasvar')\n",
    "print(acc_cnn_noniid_r10_ep10)\n",
    "np.save('/u/student/2020/cs20btech11046/resnet/new/dataset/results/acc_cnn_noniid_clt20_ep20_biasvar.npy', acc_cnn_noniid_r10_ep10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_num = 10\n",
    "best_val_acc = 0\n",
    "total_loss_val, total_acc_val = [],[]\n",
    "for epoch in range(1, epoch_num+1):\n",
    "    loss_train, acc_train = train(train_loader, model, criterion, optimizer, epoch)\n",
    "    loss_val, acc_val = validate(val_loader, model, criterion, optimizer, epoch)\n",
    "    total_loss_val.append(loss_val)\n",
    "    total_acc_val.append(acc_val)\n",
    "    if acc_val > best_val_acc:\n",
    "        best_val_acc = acc_val\n",
    "        print('*****************************************************')\n",
    "        print('best record: [epoch %d], [val loss %.5f], [val acc %.5f]' % (epoch, loss_val, acc_val))\n",
    "        print('*****************************************************')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A-BmfL-mhirk"
   },
   "outputs": [],
   "source": [
    "#a class NonIIDMyDataset is created to access and transform data\n",
    "class NonIIDMyDataset(Dataset):\n",
    "    #the __init__ function in Python is like the C++ constructor in an object-oriented approach\n",
    "    def __init__(self, data, targets, transform=None):\n",
    "        self.data = data  #data is X\n",
    "        self.targets = torch.LongTensor(targets)  #tragets are y. Convert y to a tensor to be able to used torch function on them\n",
    "        self.transform = transform  #this is the transformation to be applied on to X. By default the value is None.\n",
    "                                    #In the 2nd cell below you can see the exact transform used in the code\n",
    "\n",
    "    #this function is used to apply a transformation (if any) to X and return the pair (X, y) based on the index passed\n",
    "    def __getitem__(self, index):\n",
    "        x = self.data[index]\n",
    "        y = self.targets[index]\n",
    "\n",
    "        if self.transform:\n",
    "            # x = Image.fromarray(self.data[index].astype(np.uint8).transpose(1,2,0))\n",
    "            x = self.transform(x)\n",
    "\n",
    "        return x, y\n",
    "\n",
    "    #this function is used to get the length/no.of features of X\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "#The functions with __ at the front and back of every function in Python are called Magic Methods/Dunder Methods.\n",
    "#Magic methods are not meant to be invoked directly by you, but the invocation happens internally from the class on a certain action.\n",
    "#Not sure of the meaning but may be this concept is understood better if you find where these methods are used int he code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WRYOEMIuNpBe"
   },
   "outputs": [],
   "source": [
    "#these are the locations of train and test data for 20 clients used in the code\n",
    "#there are many other folders as well in this dataset folder. May be different ones are used for different cases\n",
    "train_location = '/u/student/2020/cs20btech11046/resnet/old/dataset/practical/20/train/'\n",
    "test_location = '/u/student/2020/cs20btech11046/resnet/old/dataset/practical/20/test/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "stJUm9zgevMl"
   },
   "outputs": [],
   "source": [
    "#the transforms library imported above, is used to create a transformation of the data(X)\n",
    "#transforms.Compose - to put more than one sequantial transforms into one\n",
    "#transforms.ToTensor - to convert a list/np array to a tensor\n",
    "#transform.Normalize - transforms.Normalize(mean, std, inplace=False) to normalize a tensor with give mean and std\n",
    "#to normalize a data means changinf x to (x-mean)/std\n",
    "#here mean is 0.137 and std is 0.3081. May be these values are obtained by calculating mean and std of X separately or they are random. Not sure\n",
    "\n",
    "#how did these value we got\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "])\n",
    "\n",
    "client_loader = []  #this list is used to store the train data loaded using 'DataLoader' module from torch in batches\n",
    "\n",
    "#this function converts y of data to a tensor using __init__, applies the above transformation to x using the __getitenm__ function in the NonIIDMyDataset\n",
    "#and loads the data in batches (in order to train it with a neural network later) and stores the loaded data into client_loader list created above\n",
    "def noniid_train_loader(bsz=10):\n",
    "  #for all the 20 clients\n",
    "  for i in range(no_clients):\n",
    "    #go to the folder /content/drive/MyDrive/dataset/practical/20/train/, read the file from client_num.npz (liek 1.npz, 2.npz ... 20.npz) and store the X and y values\n",
    "    file_path = str(i)+'.npz'\n",
    "    loc = train_location + file_path\n",
    "    data = np.load(loc)\n",
    "    X = list(data['features'])\n",
    "    Y = list(data['labels'])\n",
    "\n",
    "    #create an object called dataset which is an instance of the class NonIIDMyDataset\n",
    "    dataset = NonIIDMyDataset(X, Y, transform=transform)\n",
    "    #in batches of 10, load the whole dataset and store it in client_load\n",
    "    client_load = torch.utils.data.DataLoader(dataset, batch_size=bsz, shuffle=True)\n",
    "\n",
    "    #append every client's dataload into client_loader list\n",
    "    client_loader.append(client_load)\n",
    "\n",
    "  print(client_loader)  #you can see 20 objects of torch dataloaders\n",
    "  return client_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zehfiWjBfCeA",
    "outputId": "d7fc28de-a0ba-4828-9231-3a7fde909831"
   },
   "outputs": [],
   "source": [
    "noniid_client_train_loader = noniid_train_loader(bsz = bsz) #call the above funtion to perform all the actions explained inside the func, noniid_train_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N628K2e5M8HK"
   },
   "outputs": [],
   "source": [
    "#the exact same thing as in the func noniid_train_loader is done here. Expect that the data is extracted now read from the loacation /content/drive/MyDrive/dataset/practical/20/test\n",
    "test_loader = []\n",
    "def noniid_test_loader(batch_size,shuffle):\n",
    "  for i in range(no_clients):\n",
    "    file_path = str(i)+'.npz'\n",
    "    loc = test_location + file_path\n",
    "    data = np.load(loc)\n",
    "    X = list(data['features'])\n",
    "    Y = list(data['labels'])\n",
    "\n",
    "    dataset = NonIIDMyDataset(X, Y, transform=transform)\n",
    "    client_load = torch.utils.data.DataLoader(dataset, batch_size=bsz, shuffle=True)\n",
    "\n",
    "    test_loader.append(client_load)\n",
    "\n",
    "  print(test_loader)\n",
    "  return test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uij69lidL_1b",
    "outputId": "8c86f2ac-b7d3-4b56-e468-9bcbe74e1d75"
   },
   "outputs": [],
   "source": [
    "test_loader = noniid_test_loader(batch_size = 1000, shuffle=False)  #test data is tranformed loaded in batches of 1000 and stored in test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uvdAGMQpfKj1",
    "outputId": "5b513ea5-1c45-412a-d011-db15a59f71d4"
   },
   "outputs": [],
   "source": [
    "# non-iid\n",
    "#this cell is totally just for observation\n",
    "label_dist = torch.zeros(10)  #since we have 10 classes, create a torch array with 10 zeros\n",
    "print(type(noniid_client_train_loader[0]))\n",
    "print(noniid_client_train_loader[0])\n",
    "\n",
    "#using a for-loop, count the no.of rows is dataset which has 10 classes respectively for client 1\n",
    "for (x,y) in noniid_client_train_loader[0]:\n",
    "    label_dist+= torch.sum(F.one_hot(y,num_classes=10), dim=0)  #one-hot encoding is explained int he next cell\n",
    "\n",
    "print(\"non-iid: \", label_dist)\n",
    "#I suppose there should be a line like label_dist = torch.zeros(10) here as well\n",
    "for (x,y) in test_loader[0]:\n",
    "\n",
    "    label_dist+= torch.sum(F.one_hot(y,num_classes=10), dim=0)\n",
    "\n",
    "print(\"non-iid: \", label_dist)\n",
    "# fig, axes = plt.subplots(1, 2, figsize=(10, 4))\n",
    "# for i, ax in enumerate(axes.flat):\n",
    "#     ax.axis(\"off\")\n",
    "#     ax.set_title(label[i].cpu().numpy())\n",
    "#     ax.imshow(img[i][0], cmap=\"gray\")\n",
    "# IPython.display.display(fig)\n",
    "# plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IlmXWOTb0c7u"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "one hot encoding is a concept where we assign 1 for the class of that row and 0 for the rest\n",
    "example say we have 5 classes in the dataset.\n",
    "The classes of say 10 rows of data are 1 3 2 4 1 5 3 2 1 4. (i.e., 1st row of data belongs to class 1 ...)\n",
    "After applying one hot encoding the classes of these 10 rows will be represented as\n",
    "1th row : 1 0 0 0 0\n",
    "2th row : 0 0 1 0 0\n",
    "3th row : 0 1 0 0 0\n",
    "4th row : 0 0 0 1 0\n",
    "5th row : 1 0 0 0 0\n",
    "6th row : 0 0 0 0 1\n",
    "7th row : 0 0 1 0 0\n",
    "8th row : 0 1 0 0 0\n",
    "9th row : 1 0 0 0 0\n",
    "10th row: 0 0 0 1 0\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LPO0winNG0Vn"
   },
   "outputs": [],
   "source": [
    "#this function is only used to observe how many parameters are used in the neural network we create. It is only for observation. Not to effect the running of any code\n",
    "#parameters in neural networks are like no.of weights or bias params included to the network. Check it out on the internet\n",
    "def num_params(model):\n",
    "    \"\"\" \"\"\"\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Kci2Pvx6kExd",
    "outputId": "df79c56c-1da8-4bef-990b-e4bca95ef123"
   },
   "outputs": [],
   "source": [
    "# define fully connected NN\n",
    "#this version of Multi Layer Perceptron is not to train data in the code. Instead, the CNN defined in the next cell is used\n",
    "#the network here is: x -> linear layer -> relu activation -> linear layer -> relu activation -> linear -> output\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(784, 200);\n",
    "        self.fc2 = nn.Linear(200, 200);\n",
    "        self.out = nn.Linear(200, 10);\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.flatten(1) # [B x 784]\n",
    "        x = F.relu(self.fc1(x)) # [B x 200]\n",
    "        x = F.relu(self.fc2(x)) # [B x 200]\n",
    "        x = self.out(x) # [B x 10]\n",
    "        return x\n",
    "\n",
    "print(MLP())\n",
    "print(num_params(MLP()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "q6THHwG8k8tQ",
    "outputId": "24fc07ed-44f7-4c66-f666-11dcf2833918"
   },
   "outputs": [],
   "source": [
    "# define cnn\n",
    "#A CNN (Convolutional Neural Network) is another kind of NN.\n",
    "#In MLPs, there are layers like linear layers where a linear operation like y = w.T*x+b is applied (a linear operation) followed by activation\n",
    "#Similarly, in CNN, as the name suggests, convolution is done on x (input) to get y (output) on some layers. Here kernels are used.\n",
    "#I suggest you to look through some blogs and understand practically and mathematically\n",
    "\n",
    "#the network below is: input -> convolution 2D layer -> max pool activation -> conv 2d -> max pool -> linear -> relu -> linear -> output\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, 5)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 5)\n",
    "        self.fc = nn.Linear(1024, 512)\n",
    "        self.out = nn.Linear(512, 10)\n",
    "        self.loss = 0\n",
    "        self.control = {}\n",
    "        self.delta_control = {}\n",
    "        self.delta_y = {}\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.max_pool2d(self.conv1(x), 2, 2) # [B x 32 x 12 x 12]\n",
    "        x = F.max_pool2d(self.conv2(x), 2, 2) # [B x 64 x 4 x 4]\n",
    "        x = x.flatten(1) # [B x 1024]\n",
    "        x = F.relu(self.fc(x)) # [B x 512]\n",
    "        x = self.out(x) # [B x 10]\n",
    "        return x\n",
    "\n",
    "print(CNN())\n",
    "print(num_params(CNN()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5bMA-Huh4RGO"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "The whole idea of neural network and data revolves around the below steps:\n",
    "1. Create a basic neural network be it MLP, CNN, RNN\n",
    "2. Transform & Normalize data to be able to train and validate data using the network\n",
    "3. Change the weights etc., parameters of the neural network through back propogation or any other method\n",
    "4. For the same choose a loss function and an optimizer.\n",
    "5. Repeat until you reach some fixed no.of iterations or desired result\n",
    "\n",
    "So basically train your network with initial weights and the data and predict ŷ.\n",
    "Calculate loss/error using the loss func you choose. An example is (y-ŷ).\n",
    "If the error is more, re-train the network with new weights. This is done through back propgation which is automatically done most of the times.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pkcLDjnalBgd"
   },
   "outputs": [],
   "source": [
    "#the below function is used to validate (find the percentage of correctly predicted output)\n",
    "def validate(model,client):\n",
    "    #change the model/network to evaluation mode and for the given client, predict ŷ = model(x). If ŷ=y, add 1 to correct\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for (t, (x,y)) in enumerate(test_loader[client]):\n",
    "            x = x.to(device)\n",
    "            x = x.permute(0, 2, 3, 1)\n",
    "            #print(\"x\",x.shape)\n",
    "            y = y.to(device)\n",
    "            out = model(x)\n",
    "            correct += torch.sum(torch.argmax(out, dim=1) == y).item()\n",
    "            total += x.shape[0]\n",
    "    return correct/total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WBF3a_JRlKur"
   },
   "outputs": [],
   "source": [
    "def train_client(id, client_loader, global_model, num_local_epochs, lr):\n",
    "    #create a deepcopy of the global model and change the network to train mode\n",
    "    local_model = copy.deepcopy(global_model)\n",
    "    local_model = local_model.to(device)\n",
    "    local_model.train()\n",
    "    optimizer = torch.optim.SGD(local_model.parameters(), lr=lr)\n",
    "\n",
    "    #for given no.of epochs (iterations) run the for-loop on the client\n",
    "    for epoch in range(num_local_epochs):\n",
    "        #for every pair of (X, y), predict ŷ for X using the local model, find the loss (using cross entropy loss here) which is l\n",
    "        #predict ŷ for X using the global model, find the loss with this ŷ and y which is dl\n",
    "        #I guess this is some new formula in this version. find the total loss for present (X, y) using the formula\n",
    "        #(1-p)*loss + p*drift_loss where p = l/(l+dl+epsilon)\n",
    "        #with this loss, as said in the 2nd cell above, we perform back propogation and optimize.\n",
    "        #I am exactly not sure of the math or steps which happen in back prop and optimization\n",
    "        #You can check it online. You may get an idea\n",
    "        for (i, (x,y)) in enumerate(client_loader):\n",
    "            x = x.to(device)\n",
    "\n",
    "            #x.reshape(10, 1, 28, 28)\n",
    "            x = x.permute(0, 2, 3, 1)\n",
    "            #print(\"x\",x.shape)\n",
    "            y = y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            out = local_model(x)\n",
    "            loss = criterion(out, y)\n",
    "            global_output_batch = global_model(x)\n",
    "            drift_loss = criterion(global_output_batch,y)\n",
    "            l = loss.item()#print('loss',loss.item())\n",
    "            dl = drift_loss.item()\n",
    "            #print('drift loss',drift_loss.item())\n",
    "            p = l/(l+dl+ epsilon)\n",
    "            #print(p)\n",
    "            total_loss = (1-p)* loss+ p*drift_loss\n",
    "            total_loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    #after training the local model with the client's data for given no.of epochs, we return the local model\n",
    "    #So here, the overall model (like the server model) as effects the weights/params of local model\n",
    "    #cx every client has their own global model and local model like in our prev sem code\n",
    "    #remember we used to approach to remove the usage of server completely?\n",
    "    return local_model\n",
    "\n",
    "#this func is same as aggregation func in the prev sems code. models argument is an array of local models of all the clients\n",
    "def running_model_avg(models,client_no):\n",
    "    # Initialize the aggregated model's state dictionary\n",
    "    aggregated_state_dict = models[client_no].state_dict()\n",
    "    print(aggregated_state_dict)\n",
    "    it = 0\n",
    "    # Iterate over the models of the clients\n",
    "    for client_model in models:\n",
    "        #if the clinet_model is not the one passed as an argument\n",
    "        if it!=client_no:\n",
    "          # Get the state dictionary of the current client model\n",
    "          client_state_dict = client_model.state_dict()\n",
    "\n",
    "          # Iterate over the parameters in the state dictionary\n",
    "          for param_name, param in client_state_dict.items():\n",
    "              # Perform aggregation for each parameter\n",
    "              if param_name.endswith(\".weight\") or param_name.endswith(\".bias\"):\n",
    "                  # Update the aggregated parameter by averaging\n",
    "                  aggregated_state_dict[param_name] += param\n",
    "        it = it + 1\n",
    "\n",
    "    # Compute the average by dividing by the number of client models\n",
    "    num_client_models = len(models)\n",
    "#     print(num_client_models)\n",
    "    for param_name in aggregated_state_dict:\n",
    "#         print(param_name)\n",
    "#         print(aggregated_state_dict[param_name])\n",
    "        if param_name.endswith(\".weight\") or param_name.endswith(\".bias\"):\n",
    "            aggregated_state_dict[param_name] /= num_client_models\n",
    "\n",
    "    # Create a new model instance for the aggregated model\n",
    "    aggregated_model = type(models[0])()\n",
    "    aggregated_model.load_state_dict(aggregated_state_dict)\n",
    "\n",
    "    return aggregated_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_fv2VaSF9X5Q"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Basically what is happening above is that, create a state_dict called aggregated_state_dict and intitialize it with our present client's data\n",
    "Now run a for loop through all the other client's model and add their params (state_dict) to the aggregated_state_dict\n",
    "Then to normalize it, divide all the params of aggreated_state_dict by no.of clients (20 here)\n",
    "Then create a model structure for aggregated_model and load all these params into this.\n",
    "\n",
    "But ig we can just add the params of all clients directly instead of that if it!=client_no statement. Cz at the end I feel we are just adding the params of all the local models.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Cw0lZVFflOSL"
   },
   "outputs": [],
   "source": [
    "#modified scaffold each client having their own global model global_model[0] for client[0]\n",
    "\n",
    "def scaffold_experiment(global_model, num_clients, num_local_epochs, lr, client_train_loader, max_rounds, filename):   ##num_client\n",
    "    round_accuracy = []\n",
    "    #for all the rounds\n",
    "    for t in range(max_rounds):\n",
    "        print(\"starting round {}\".format(t))\n",
    "\n",
    "        # choose clients\n",
    "        #clients = np.random.choice(np.arange(100), num_clients, replace = False)  ###remove this\n",
    "        clients=num_clients\n",
    "        print(\"clients: \", clients)   ##\n",
    "        #create 2 list to store global (aggregated) and local models of each client\n",
    "        running_avg = [None for _ in range(clients)]\n",
    "        local_models = [None for _ in range(clients)]\n",
    "\n",
    "        for i in range(clients):\n",
    "          global_model[i].eval()\n",
    "          global_model[i] = global_model[i].to(device)\n",
    "          #running_avg = np.empty(clients, dtype='collections.OrderedDict') #None\n",
    "\n",
    "        #for all the clients, train their local models with their dataset\n",
    "        for i in range(clients):\n",
    "            # train local client\n",
    "            print(\"round {}, starting client {}/{}, id: {}\".format(t, i+1,num_clients, i+1))\n",
    "            local_models[i] = train_client(i, client_train_loader[i], global_model[i], num_local_epochs, lr)\n",
    "\n",
    "        # add local model parameters to running average of each client\n",
    "        for j in range(clients):\n",
    "          #print(type(running_avg[j]), '   and ',type(local_model.state_dict()))\n",
    "          global_model[j] = running_model_avg(local_models,j)\n",
    "\n",
    "\n",
    "       # validate\n",
    "        acc_val = 0\n",
    "        for client in range(no_clients):\n",
    "          val_acc = validate(global_model[client],client)\n",
    "          #print('each client',val_acc)\n",
    "          acc_val = acc_val + val_acc\n",
    "        acc_val = acc_val/no_clients\n",
    "        print(\"round {}, validation acc: {}\".format(t, val_acc))\n",
    "        round_accuracy.append(acc_val)\n",
    "\n",
    "        if (t % 10 == 0):\n",
    "          np.save(filename+'_{}'.format(t)+'.npy', np.array(round_accuracy))\n",
    "\n",
    "    return np.array(round_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Kj2J-t3vlUPX",
    "outputId": "8b932176-23a2-4ba7-a360-5378f82ece90"
   },
   "outputs": [],
   "source": [
    "#cnn modified cnn_iid_m10 for each client seperate\n",
    "cnn = CNN() #create a CNN object\n",
    "print(cnn)\n",
    "print(\"total params: \", num_params(cnn))\n",
    "# CNN - iid - m=10 experiment\n",
    "cnn_noniid_r10_ep10 = np.empty(no_clients, dtype=CNN) #create a CNN with uninitialized and random params (weights and bias) with size 20\n",
    "\n",
    "#for all the 20 clients, make a deepcopy of cnn created in the first line of this cell (initializing the empty array created above)\n",
    "#these will be the global models for all the 20 clients\n",
    "for i in range(no_clients):\n",
    "  cnn_noniid_r10_ep10[i] = copy.deepcopy(cnn)\n",
    "\n",
    "#cnn_iid_m10 = copy.deepcopy(cnn)\n",
    "#calculate the accuracies obtained by the final aggregated global model for all the client's datasets\n",
    "#we are passing, global models of all clients, no.of clients, no.of epochs, learning rate for optimizer,\n",
    "#the train data of all clients loaded (at the beginning of this notebook), no.of rounds to run the exp, filepath to store the results\n",
    "acc_cnn_noniid_r10_ep10 = scaffold_experiment(cnn_noniid_r10_ep10, num_clients=no_clients,\n",
    "                                 num_local_epochs=5,\n",
    "                                 lr=0.01,\n",
    "                                 client_train_loader = noniid_client_train_loader,\n",
    "                                 max_rounds=10,  ## for testing keep it 1\n",
    "                                 filename='/u/student/2020/cs20btech11046/resnet/old/dataset/results/acc_cnn_noniid_clt20_e20_biasvar')\n",
    "print(acc_cnn_noniid_r10_ep10)\n",
    "np.save('/u/student/2020/cs20btech11046/resnet/old/dataset/results/acc_cnn_noniid_clt20_ep20_biasvar.npy', acc_cnn_noniid_r10_ep10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gaAEiZsdTHwD"
   },
   "outputs": [],
   "source": [
    "#this function is not use anywhere in the code\n",
    "def view_10(img, label):\n",
    "    \"\"\" view 10 labelled examples from tensor\"\"\"\n",
    "    fig, axes = plt.subplots(2, 5, figsize=(10, 4))\n",
    "    for i, ax in enumerate(axes.flat):\n",
    "        ax.axis(\"off\")\n",
    "        ax.set_title(label[i].cpu().numpy())\n",
    "        ax.imshow(img[i][0], cmap=\"gray\")\n",
    "    IPython.display.display(fig)\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "smplc98wIWKv"
   },
   "outputs": [],
   "source": [
    " acc_cnn_noniid_r10_ep10 = np.load('/u/student/2020/cs20btech11046/resnet/old/dataset/results/acc_cnn_noniid_r10_ep10_bias9.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F8EdqdNVI5sV"
   },
   "outputs": [],
   "source": [
    "x = np.arange(0,15)\n",
    "plt.figure(figsize=(8,6))\n",
    "\n",
    "plt.title(\"Scaffold test accuracy after $t$ rounds on non-iid MNIST\")\n",
    "\n",
    "plt.xlabel(\"Communication rounds $t$\")\n",
    "plt.ylabel(\"Test accuracy\")\n",
    "plt.axis([0, 15, 0.3, 1])\n",
    "\n",
    "\n",
    "\n",
    "plt.axhline(y=0.7, color='r', linestyle='dashed')\n",
    "plt.axhline(y=0.9, color='b', linestyle='dashed')\n",
    "\n",
    "plt.plot(x, acc_cnn_noniid_r10_ep10, label='2NN, $m=10$, $E=1$')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eOSMoo4W6Y2c"
   },
   "outputs": [],
   "source": [
    "acc_cnn_noniid_r10_ep10_b = np.load('/u/student/2020/cs20btech11046/resnet/old/dataset/results/acc_cnn_noniid_r10_ep10_bias8.npy')\n",
    "acc_cnn_noniid_r10_ep10_wb = np.load('/u/student/2020/cs20btech11046/resnet/old/dataset/results/acc_cnn_noniid_r10_ep10_nobias.npy')\n",
    "acc_cnn_noniid_r10_ep10_b9 = np.load('/u/student/2020/cs20btech11046/resnet/old/dataset/results/acc_cnn_noniid_r10_ep10_biasvar.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6BEq4HJb6egL"
   },
   "outputs": [],
   "source": [
    "x = np.arange(0,15)\n",
    "plt.figure(figsize=(8,6))\n",
    "\n",
    "plt.title(\"Scaffold test accuracy after $t$ rounds on non-iid MNIST\")\n",
    "\n",
    "plt.xlabel(\"Communication rounds $t$\")\n",
    "plt.ylabel(\"Test accuracy\")\n",
    "plt.axis([0, 15, 0.3, 1])\n",
    "\n",
    "\n",
    "\n",
    "plt.axhline(y=0.5, color='r', linestyle='dashed')\n",
    "plt.axhline(y=0.9, color='b', linestyle='dashed')\n",
    "\n",
    "graph1, =plt.plot(x, acc_cnn_noniid_r10_ep10_b, label='b8')\n",
    "graph2, =plt.plot(x, acc_cnn_noniid_r10_ep10_wb, label='wb')\n",
    "graph3, =plt.plot(x, acc_cnn_noniid_r10_ep10_b9, label='b9')\n",
    "plt.legend(handles=[graph1, graph2, graph3],loc =\"lower right\")\n",
    "#plt.legend([\"blue\", \"green\",\"red\"], loc =\"lower right\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
